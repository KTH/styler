{
    "error_id": "54",
    "information": {
        "errors": [
            {
                "line": "15",
                "severity": "error",
                "message": "Line is longer than 100 characters (found 114).",
                "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
            }
        ]
    },
    "source_code": "\n\n  public LINEFirstOrderModel(int dim, int negative, int seed, int maxIndex, int numNodeOneRow, float[][] layers) {\n    super(dim, negative, seed, maxIndex, numNodeOneRow, layers);\n  }\n",
    "results": [
        {
            "tool": "styler",
            "errors": [
                {
                    "line": "15",
                    "column": "42",
                    "severity": "error",
                    "message": "'(' is preceded with whitespace.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.MethodParamPadCheck"
                }
            ],
            "diff": "diff --git a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/54/LINEFirstOrderModel.java b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/styler/54/LINEFirstOrderModel.java\nindex 4af089cbb9..081fa2b45b 100644\n--- a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/54/LINEFirstOrderModel.java\n+++ b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/styler/54/LINEFirstOrderModel.java\n@@ -11,103 +11,102 @@ import java.util.Random;\n \n public class LINEFirstOrderModel extends EmbeddingModel {\n \n-\n-  public LINEFirstOrderModel(int dim, int negative, int seed, int maxIndex, int numNodeOneRow, float[][] layers) {\n-    super(dim, negative, seed, maxIndex, numNodeOneRow, layers);\n-  }\n-\n-  @Override\n-  public float[] dot(ByteBuf edges) {\n-\n-    Random negativeSeed = new Random(seed);\n-    IntOpenHashSet numInputs = new IntOpenHashSet();\n-\n-    int batchSize = edges.readInt();\n-    float[] partialDots = new float[batchSize * (1 + negative)];\n-    int dotInc = 0;\n-    for (int position = 0; position < batchSize; position++) {\n-      int src = edges.readInt();\n-      int dst = edges.readInt();\n-      // Skip-Gram model\n-      float[] inputs = layers[src / numNodeOneRow];\n-      int l1 = (src % numNodeOneRow) * dim;\n-      numInputs.add(src);\n-\n-      // Negative sampling\n-      int target;\n-      for (int a = 0; a < negative + 1; a++) {\n-        if (a == 0) target = dst;\n-        else do {\n-          target = negativeSeed.nextInt(maxIndex);\n-        } while (target == src);\n-\n-        numInputs.add(target);\n-        float[] outputs = layers[target / numNodeOneRow];\n-        int l2 = (target % numNodeOneRow) * dim;\n-        float f = 0.0f;\n-        for (int b = 0; b < dim; b++) f += inputs[l1 + b] * outputs[l2 + b];\n-        partialDots[dotInc++] = f;\n-      }\n-    }\n-    this.numInputsToUpdate = numInputs.size();\n-    return partialDots;\n-  }\n-\n-  @Override\n-  public void adjust(ByteBuf dataBuf, int numInputs, int numOutputs) {\n-\n-    // used to accumulate the updates for input vectors\n-    float[] neu1e = new float[dim];\n-    float[] inputUpdates = new float[numInputs * dim];\n-    Int2IntOpenHashMap inputIndex = new Int2IntOpenHashMap();\n-    Int2IntOpenHashMap inputUpdateCounter = new Int2IntOpenHashMap();\n-    Random negativeSeed = new Random(seed);\n-    int batchSize = dataBuf.readInt();\n-\n-    for (int position = 0; position < batchSize; position++) {\n-      int src = dataBuf.readInt();\n-      int dst = dataBuf.readInt();\n-\n-      float[] inputs = layers[src / numNodeOneRow];\n-      int l1 = (src % numNodeOneRow) * dim;\n-\n-      Arrays.fill(neu1e, 0);\n-\n-      // Negative sampling\n-      int target;\n-      for (int d = 0; d < negative + 1; d++) {\n-        if (d == 0) target = dst;\n-        else do {\n-          target = negativeSeed.nextInt(maxIndex);\n-        } while (target == src);\n-\n-        float[] outputs = layers[target / numNodeOneRow];\n-        int l2 = (target % numNodeOneRow) * dim;\n-\n-        float g = dataBuf.readFloat();\n-\n-        // accumulate for the hidden layer\n-        for (int a = 0; a < dim; a++) neu1e[a] += g * outputs[a + l2];\n-        // update output layer\n-        merge(inputUpdates, inputIndex, target, inputs, g, l1);\n-        inputUpdateCounter.addTo(target, 1);\n-      }\n-\n-      // update the hidden layer\n-      merge(inputUpdates, inputIndex, src, neu1e, 1, 0);\n-      inputUpdateCounter.addTo(src, 1);\n-    }\n-\n-    // update input\n-    ObjectIterator<Int2IntMap.Entry> it = inputIndex.int2IntEntrySet().fastIterator();\n-    while (it.hasNext()) {\n-      Int2IntMap.Entry entry = it.next();\n-      int node = entry.getIntKey();\n-      int offset = entry.getIntValue() * dim;\n-      int divider = inputUpdateCounter.get(node);\n-      int col = (node % numNodeOneRow) * dim;\n-      float[] values = layers[node / numNodeOneRow];\n-      for (int a = 0; a < dim; a++) values[a + col] += inputUpdates[offset + a] / divider;\n-    }\n-  }\n+  public LINEFirstOrderModel(int dim, int negative, int seed, int maxIndex, int\n+  numNodeOneRow,float[ ] []layers){super (dim, negative, seed, maxIndex, numNodeOneRow, layers);\n+ }\n+\n+ @Override\n+ public float[] dot(ByteBuf edges) {\n+\n+   Random negativeSeed = new Random(seed);\n+   IntOpenHashSet numInputs = new IntOpenHashSet();\n+\n+   int batchSize = edges.readInt();\n+   float[] partialDots = new float[batchSize * (1 + negative)];\n+   int dotInc = 0;\n+   for (int position = 0; position < batchSize; position++) {\n+     int src = edges.readInt();\n+     int dst = edges.readInt();\n+     // Skip-Gram model\n+     float[] inputs = layers[src / numNodeOneRow];\n+     int l1 = (src % numNodeOneRow) * dim;\n+     numInputs.add(src);\n+\n+     // Negative sampling\n+     int target;\n+     for (int a = 0; a < negative + 1; a++) {\n+       if (a == 0) target = dst;\n+       else do {\n+         target = negativeSeed.nextInt(maxIndex);\n+       } while (target == src);\n+\n+       numInputs.add(target);\n+       float[] outputs = layers[target / numNodeOneRow];\n+       int l2 = (target % numNodeOneRow) * dim;\n+       float f = 0.0f;\n+       for (int b = 0; b < dim; b++) f += inputs[l1 + b] * outputs[l2 + b];\n+       partialDots[dotInc++] = f;\n+     }\n+   }\n+   this.numInputsToUpdate = numInputs.size();\n+   return partialDots;\n+ }\n+\n+ @Override\n+ public void adjust(ByteBuf dataBuf, int numInputs, int numOutputs) {\n+\n+   // used to accumulate the updates for input vectors\n+   float[] neu1e = new float[dim];\n+   float[] inputUpdates = new float[numInputs * dim];\n+   Int2IntOpenHashMap inputIndex = new Int2IntOpenHashMap();\n+   Int2IntOpenHashMap inputUpdateCounter = new Int2IntOpenHashMap();\n+   Random negativeSeed = new Random(seed);\n+   int batchSize = dataBuf.readInt();\n+\n+   for (int position = 0; position < batchSize; position++) {\n+     int src = dataBuf.readInt();\n+     int dst = dataBuf.readInt();\n+\n+     float[] inputs = layers[src / numNodeOneRow];\n+     int l1 = (src % numNodeOneRow) * dim;\n+\n+     Arrays.fill(neu1e, 0);\n+\n+     // Negative sampling\n+     int target;\n+     for (int d = 0; d < negative + 1; d++) {\n+       if (d == 0) target = dst;\n+       else do {\n+         target = negativeSeed.nextInt(maxIndex);\n+       } while (target == src);\n+\n+       float[] outputs = layers[target / numNodeOneRow];\n+       int l2 = (target % numNodeOneRow) * dim;\n+\n+       float g = dataBuf.readFloat();\n+\n+       // accumulate for the hidden layer\n+       for (int a = 0; a < dim; a++) neu1e[a] += g * outputs[a + l2];\n+       // update output layer\n+       merge(inputUpdates, inputIndex, target, inputs, g, l1);\n+       inputUpdateCounter.addTo(target, 1);\n+     }\n+\n+     // update the hidden layer\n+     merge(inputUpdates, inputIndex, src, neu1e, 1, 0);\n+     inputUpdateCounter.addTo(src, 1);\n+   }\n+\n+   // update input\n+   ObjectIterator<Int2IntMap.Entry> it = inputIndex.int2IntEntrySet().fastIterator();\n+   while (it.hasNext()) {\n+     Int2IntMap.Entry entry = it.next();\n+     int node = entry.getIntKey();\n+     int offset = entry.getIntValue() * dim;\n+     int divider = inputUpdateCounter.get(node);\n+     int col = (node % numNodeOneRow) * dim;\n+     float[] values = layers[node / numNodeOneRow];\n+     for (int a = 0; a < dim; a++) values[a + col] += inputUpdates[offset + a] / divider;\n+   }\n+ }\n }\n",
            "diff_size": 99
        },
        {
            "tool": "intellij",
            "errors": [],
            "diff": "diff --git a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/54/LINEFirstOrderModel.java b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/intellij/54/LINEFirstOrderModel.java\nindex 4af089cbb9..34143caa95 100644\n--- a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/54/LINEFirstOrderModel.java\n+++ b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/intellij/54/LINEFirstOrderModel.java\n@@ -12,102 +12,109 @@ import java.util.Random;\n public class LINEFirstOrderModel extends EmbeddingModel {\n \n \n-  public LINEFirstOrderModel(int dim, int negative, int seed, int maxIndex, int numNodeOneRow, float[][] layers) {\n-    super(dim, negative, seed, maxIndex, numNodeOneRow, layers);\n-  }\n-\n-  @Override\n-  public float[] dot(ByteBuf edges) {\n-\n-    Random negativeSeed = new Random(seed);\n-    IntOpenHashSet numInputs = new IntOpenHashSet();\n-\n-    int batchSize = edges.readInt();\n-    float[] partialDots = new float[batchSize * (1 + negative)];\n-    int dotInc = 0;\n-    for (int position = 0; position < batchSize; position++) {\n-      int src = edges.readInt();\n-      int dst = edges.readInt();\n-      // Skip-Gram model\n-      float[] inputs = layers[src / numNodeOneRow];\n-      int l1 = (src % numNodeOneRow) * dim;\n-      numInputs.add(src);\n-\n-      // Negative sampling\n-      int target;\n-      for (int a = 0; a < negative + 1; a++) {\n-        if (a == 0) target = dst;\n-        else do {\n-          target = negativeSeed.nextInt(maxIndex);\n-        } while (target == src);\n-\n-        numInputs.add(target);\n-        float[] outputs = layers[target / numNodeOneRow];\n-        int l2 = (target % numNodeOneRow) * dim;\n-        float f = 0.0f;\n-        for (int b = 0; b < dim; b++) f += inputs[l1 + b] * outputs[l2 + b];\n-        partialDots[dotInc++] = f;\n-      }\n+    public LINEFirstOrderModel(int dim, int negative, int seed, int maxIndex, int numNodeOneRow,\n+                               float[][] layers) {\n+        super(dim, negative, seed, maxIndex, numNodeOneRow, layers);\n     }\n-    this.numInputsToUpdate = numInputs.size();\n-    return partialDots;\n-  }\n-\n-  @Override\n-  public void adjust(ByteBuf dataBuf, int numInputs, int numOutputs) {\n-\n-    // used to accumulate the updates for input vectors\n-    float[] neu1e = new float[dim];\n-    float[] inputUpdates = new float[numInputs * dim];\n-    Int2IntOpenHashMap inputIndex = new Int2IntOpenHashMap();\n-    Int2IntOpenHashMap inputUpdateCounter = new Int2IntOpenHashMap();\n-    Random negativeSeed = new Random(seed);\n-    int batchSize = dataBuf.readInt();\n-\n-    for (int position = 0; position < batchSize; position++) {\n-      int src = dataBuf.readInt();\n-      int dst = dataBuf.readInt();\n-\n-      float[] inputs = layers[src / numNodeOneRow];\n-      int l1 = (src % numNodeOneRow) * dim;\n-\n-      Arrays.fill(neu1e, 0);\n-\n-      // Negative sampling\n-      int target;\n-      for (int d = 0; d < negative + 1; d++) {\n-        if (d == 0) target = dst;\n-        else do {\n-          target = negativeSeed.nextInt(maxIndex);\n-        } while (target == src);\n-\n-        float[] outputs = layers[target / numNodeOneRow];\n-        int l2 = (target % numNodeOneRow) * dim;\n-\n-        float g = dataBuf.readFloat();\n-\n-        // accumulate for the hidden layer\n-        for (int a = 0; a < dim; a++) neu1e[a] += g * outputs[a + l2];\n-        // update output layer\n-        merge(inputUpdates, inputIndex, target, inputs, g, l1);\n-        inputUpdateCounter.addTo(target, 1);\n-      }\n-\n-      // update the hidden layer\n-      merge(inputUpdates, inputIndex, src, neu1e, 1, 0);\n-      inputUpdateCounter.addTo(src, 1);\n+\n+    @Override\n+    public float[] dot(ByteBuf edges) {\n+\n+        Random negativeSeed = new Random(seed);\n+        IntOpenHashSet numInputs = new IntOpenHashSet();\n+\n+        int batchSize = edges.readInt();\n+        float[] partialDots = new float[batchSize * (1 + negative)];\n+        int dotInc = 0;\n+        for (int position = 0; position < batchSize; position++) {\n+            int src = edges.readInt();\n+            int dst = edges.readInt();\n+            // Skip-Gram model\n+            float[] inputs = layers[src / numNodeOneRow];\n+            int l1 = (src % numNodeOneRow) * dim;\n+            numInputs.add(src);\n+\n+            // Negative sampling\n+            int target;\n+            for (int a = 0; a < negative + 1; a++) {\n+                if (a == 0) {\n+                    target = dst;\n+                } else {\n+                    do {\n+                        target = negativeSeed.nextInt(maxIndex);\n+                    } while (target == src);\n+                }\n+\n+                numInputs.add(target);\n+                float[] outputs = layers[target / numNodeOneRow];\n+                int l2 = (target % numNodeOneRow) * dim;\n+                float f = 0.0f;\n+                for (int b = 0; b < dim; b++) f += inputs[l1 + b] * outputs[l2 + b];\n+                partialDots[dotInc++] = f;\n+            }\n+        }\n+        this.numInputsToUpdate = numInputs.size();\n+        return partialDots;\n     }\n \n-    // update input\n-    ObjectIterator<Int2IntMap.Entry> it = inputIndex.int2IntEntrySet().fastIterator();\n-    while (it.hasNext()) {\n-      Int2IntMap.Entry entry = it.next();\n-      int node = entry.getIntKey();\n-      int offset = entry.getIntValue() * dim;\n-      int divider = inputUpdateCounter.get(node);\n-      int col = (node % numNodeOneRow) * dim;\n-      float[] values = layers[node / numNodeOneRow];\n-      for (int a = 0; a < dim; a++) values[a + col] += inputUpdates[offset + a] / divider;\n+    @Override\n+    public void adjust(ByteBuf dataBuf, int numInputs, int numOutputs) {\n+\n+        // used to accumulate the updates for input vectors\n+        float[] neu1e = new float[dim];\n+        float[] inputUpdates = new float[numInputs * dim];\n+        Int2IntOpenHashMap inputIndex = new Int2IntOpenHashMap();\n+        Int2IntOpenHashMap inputUpdateCounter = new Int2IntOpenHashMap();\n+        Random negativeSeed = new Random(seed);\n+        int batchSize = dataBuf.readInt();\n+\n+        for (int position = 0; position < batchSize; position++) {\n+            int src = dataBuf.readInt();\n+            int dst = dataBuf.readInt();\n+\n+            float[] inputs = layers[src / numNodeOneRow];\n+            int l1 = (src % numNodeOneRow) * dim;\n+\n+            Arrays.fill(neu1e, 0);\n+\n+            // Negative sampling\n+            int target;\n+            for (int d = 0; d < negative + 1; d++) {\n+                if (d == 0) {\n+                    target = dst;\n+                } else {\n+                    do {\n+                        target = negativeSeed.nextInt(maxIndex);\n+                    } while (target == src);\n+                }\n+\n+                float[] outputs = layers[target / numNodeOneRow];\n+                int l2 = (target % numNodeOneRow) * dim;\n+\n+                float g = dataBuf.readFloat();\n+\n+                // accumulate for the hidden layer\n+                for (int a = 0; a < dim; a++) neu1e[a] += g * outputs[a + l2];\n+                // update output layer\n+                merge(inputUpdates, inputIndex, target, inputs, g, l1);\n+                inputUpdateCounter.addTo(target, 1);\n+            }\n+\n+            // update the hidden layer\n+            merge(inputUpdates, inputIndex, src, neu1e, 1, 0);\n+            inputUpdateCounter.addTo(src, 1);\n+        }\n+\n+        // update input\n+        ObjectIterator<Int2IntMap.Entry> it = inputIndex.int2IntEntrySet().fastIterator();\n+        while (it.hasNext()) {\n+            Int2IntMap.Entry entry = it.next();\n+            int node = entry.getIntKey();\n+            int offset = entry.getIntValue() * dim;\n+            int divider = inputUpdateCounter.get(node);\n+            int col = (node % numNodeOneRow) * dim;\n+            float[] values = layers[node / numNodeOneRow];\n+            for (int a = 0; a < dim; a++) values[a + col] += inputUpdates[offset + a] / divider;\n+        }\n     }\n-  }\n }\n",
            "diff_size": 143
        },
        {
            "tool": "naturalize",
            "errors": [
                {
                    "line": "14",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 114).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                }
            ],
            "diff": "diff --git a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/54/LINEFirstOrderModel.java b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/naturalize/54/LINEFirstOrderModel.java\nindex 4af089cbb9..c5f3e8401d 100644\n--- a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/54/LINEFirstOrderModel.java\n+++ b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/naturalize/54/LINEFirstOrderModel.java\n@@ -11,7 +11,6 @@ import java.util.Random;\n \n public class LINEFirstOrderModel extends EmbeddingModel {\n \n-\n   public LINEFirstOrderModel(int dim, int negative, int seed, int maxIndex, int numNodeOneRow, float[][] layers) {\n     super(dim, negative, seed, maxIndex, numNodeOneRow, layers);\n   }\n@@ -37,7 +36,7 @@ public class LINEFirstOrderModel extends EmbeddingModel {\n       int target;\n       for (int a = 0; a < negative + 1; a++) {\n         if (a == 0) target = dst;\n-        else do {\n+  else do {\n           target = negativeSeed.nextInt(maxIndex);\n         } while (target == src);\n \n@@ -45,7 +44,8 @@ public class LINEFirstOrderModel extends EmbeddingModel {\n         float[] outputs = layers[target / numNodeOneRow];\n         int l2 = (target % numNodeOneRow) * dim;\n         float f = 0.0f;\n-        for (int b = 0; b < dim; b++) f += inputs[l1 + b] * outputs[l2 + b];\n+        for (int b = 0; b < dim; b++)\n+  f += inputs[l1 + b] * outputs[l2 + b];\n         partialDots[dotInc++] = f;\n       }\n     }\n@@ -77,7 +77,7 @@ public class LINEFirstOrderModel extends EmbeddingModel {\n       int target;\n       for (int d = 0; d < negative + 1; d++) {\n         if (d == 0) target = dst;\n-        else do {\n+  else do {\n           target = negativeSeed.nextInt(maxIndex);\n         } while (target == src);\n \n@@ -110,4 +110,4 @@ public class LINEFirstOrderModel extends EmbeddingModel {\n       for (int a = 0; a < dim; a++) values[a + col] += inputUpdates[offset + a] / divider;\n     }\n   }\n-}\n+}\n\\ No newline at end of file\n",
            "diff_size": 6
        },
        {
            "tool": "codebuff",
            "errors": [
                {
                    "line": "13",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 116).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "27",
                    "severity": "error",
                    "message": "Comment has incorrect indentation level 6, expected is 8, indentation should be the same level as line 28.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.CommentsIndentationCheck"
                },
                {
                    "line": "32",
                    "severity": "error",
                    "message": "Comment has incorrect indentation level 6, expected is 8, indentation should be the same level as line 34.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.CommentsIndentationCheck"
                },
                {
                    "line": "40",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 101).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "42",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 114).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "43",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 105).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "74",
                    "severity": "error",
                    "message": "Comment has incorrect indentation level 6, expected is 8, indentation should be the same level as line 76.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.CommentsIndentationCheck"
                },
                {
                    "line": "82",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 101).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "83",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 114).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "84",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 105).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "87",
                    "severity": "error",
                    "message": "Comment has incorrect indentation level 8, expected is 65, indentation should be the same level as line 88.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.CommentsIndentationCheck"
                },
                {
                    "line": "90",
                    "severity": "error",
                    "message": "Comment has incorrect indentation level 8, expected is 65, indentation should be the same level as line 91.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.CommentsIndentationCheck"
                },
                {
                    "line": "91",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 120).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "92",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 101).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "95",
                    "severity": "error",
                    "message": "Comment has incorrect indentation level 6, expected is 8, indentation should be the same level as line 96.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.CommentsIndentationCheck"
                }
            ],
            "diff": "diff --git a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/54/LINEFirstOrderModel.java b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/codebuff/54/LINEFirstOrderModel.java\nindex 4af089cbb9..38460f9c0f 100644\n--- a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/54/LINEFirstOrderModel.java\n+++ b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/codebuff/54/LINEFirstOrderModel.java\n@@ -5,56 +5,57 @@ import it.unimi.dsi.fastutil.ints.Int2IntMap;\n import it.unimi.dsi.fastutil.ints.Int2IntOpenHashMap;\n import it.unimi.dsi.fastutil.ints.IntOpenHashSet;\n import it.unimi.dsi.fastutil.objects.ObjectIterator;\n-\n import java.util.Arrays;\n import java.util.Random;\n \n public class LINEFirstOrderModel extends EmbeddingModel {\n \n-\n-  public LINEFirstOrderModel(int dim, int negative, int seed, int maxIndex, int numNodeOneRow, float[][] layers) {\n+    public LINEFirstOrderModel(int dim, int negative, int seed, int maxIndex, int numNodeOneRow, float[][] layers) {\n     super(dim, negative, seed, maxIndex, numNodeOneRow, layers);\n-  }\n-\n-  @Override\n-  public float[] dot(ByteBuf edges) {\n+    }\n \n+    @Override\n+    public float[] dot(ByteBuf edges) {\n     Random negativeSeed = new Random(seed);\n     IntOpenHashSet numInputs = new IntOpenHashSet();\n-\n     int batchSize = edges.readInt();\n     float[] partialDots = new float[batchSize * (1 + negative)];\n     int dotInc = 0;\n     for (int position = 0; position < batchSize; position++) {\n-      int src = edges.readInt();\n-      int dst = edges.readInt();\n+        int src = edges.readInt();\n+        int dst = edges.readInt();\n       // Skip-Gram model\n-      float[] inputs = layers[src / numNodeOneRow];\n-      int l1 = (src % numNodeOneRow) * dim;\n-      numInputs.add(src);\n+        float[] inputs = layers[src / numNodeOneRow];\n+        int l1 = (src % numNodeOneRow) * dim;\n+        numInputs.add(src);\n \n       // Negative sampling\n-      int target;\n-      for (int a = 0; a < negative + 1; a++) {\n-        if (a == 0) target = dst;\n-        else do {\n-          target = negativeSeed.nextInt(maxIndex);\n-        } while (target == src);\n-\n-        numInputs.add(target);\n-        float[] outputs = layers[target / numNodeOneRow];\n-        int l2 = (target % numNodeOneRow) * dim;\n-        float f = 0.0f;\n-        for (int b = 0; b < dim; b++) f += inputs[l1 + b] * outputs[l2 + b];\n-        partialDots[dotInc++] = f;\n-      }\n+\n+        int target;\n+        for (int a = 0; a < negative + 1; a++) {\n+                                                                 if (a == 0) target = dst;\n+                                                                 else\n+                                                                             do {\n+                                                   target = negativeSeed.nextInt(maxIndex);\n+                                                                             } while (target == src);\n+                                                                 numInputs.add(target);\n+                                                                 float[] outputs = layers[target / numNodeOneRow];\n+                                                                 int l2 = (target % numNodeOneRow) * dim;\n+                                                                 float f = 0.0f;\n+                                                                 for (int b = 0; b < dim; b++)\n+                                                   f += inputs[l1 + b] * outputs[l2 + b];\n+                                                                 partialDots[dotInc++] = f;\n+        }\n     }\n+\n+\n+\n     this.numInputsToUpdate = numInputs.size();\n     return partialDots;\n-  }\n+    }\n \n-  @Override\n-  public void adjust(ByteBuf dataBuf, int numInputs, int numOutputs) {\n+    @Override\n+    public void adjust(ByteBuf dataBuf, int numInputs, int numOutputs) {\n \n     // used to accumulate the updates for input vectors\n     float[] neu1e = new float[dim];\n@@ -63,51 +64,52 @@ public class LINEFirstOrderModel extends EmbeddingModel {\n     Int2IntOpenHashMap inputUpdateCounter = new Int2IntOpenHashMap();\n     Random negativeSeed = new Random(seed);\n     int batchSize = dataBuf.readInt();\n-\n     for (int position = 0; position < batchSize; position++) {\n-      int src = dataBuf.readInt();\n-      int dst = dataBuf.readInt();\n-\n-      float[] inputs = layers[src / numNodeOneRow];\n-      int l1 = (src % numNodeOneRow) * dim;\n-\n-      Arrays.fill(neu1e, 0);\n+        int src = dataBuf.readInt();\n+        int dst = dataBuf.readInt();\n+        float[] inputs = layers[src / numNodeOneRow];\n+        int l1 = (src % numNodeOneRow) * dim;\n+        Arrays.fill(neu1e, 0);\n \n       // Negative sampling\n-      int target;\n-      for (int d = 0; d < negative + 1; d++) {\n-        if (d == 0) target = dst;\n-        else do {\n-          target = negativeSeed.nextInt(maxIndex);\n-        } while (target == src);\n \n-        float[] outputs = layers[target / numNodeOneRow];\n-        int l2 = (target % numNodeOneRow) * dim;\n-\n-        float g = dataBuf.readFloat();\n+        int target;\n+        for (int d = 0; d < negative + 1; d++) {\n+                                                                 if (d == 0) target = dst;\n+                                                                 else\n+                                                                             do {\n+                                                   target = negativeSeed.nextInt(maxIndex);\n+                                                                             } while (target == src);\n+                                                                 float[] outputs = layers[target / numNodeOneRow];\n+                                                                 int l2 = (target % numNodeOneRow) * dim;\n+                                                                 float g = dataBuf.readFloat();\n \n         // accumulate for the hidden layer\n-        for (int a = 0; a < dim; a++) neu1e[a] += g * outputs[a + l2];\n+                                                                 for (int a = 0; a < dim; a++)\n+                                                   neu1e[a] += g * outputs[a + l2];\n         // update output layer\n-        merge(inputUpdates, inputIndex, target, inputs, g, l1);\n-        inputUpdateCounter.addTo(target, 1);\n-      }\n+                                                                 merge(inputUpdates, inputIndex, target, inputs, g, l1);\n+                                                                 inputUpdateCounter.addTo(target, 1);\n+        }\n \n       // update the hidden layer\n-      merge(inputUpdates, inputIndex, src, neu1e, 1, 0);\n-      inputUpdateCounter.addTo(src, 1);\n+        merge(inputUpdates, inputIndex, src, neu1e, 1, 0);\n+        inputUpdateCounter.addTo(src, 1);\n     }\n \n     // update input\n+\n     ObjectIterator<Int2IntMap.Entry> it = inputIndex.int2IntEntrySet().fastIterator();\n     while (it.hasNext()) {\n-      Int2IntMap.Entry entry = it.next();\n-      int node = entry.getIntKey();\n-      int offset = entry.getIntValue() * dim;\n-      int divider = inputUpdateCounter.get(node);\n-      int col = (node % numNodeOneRow) * dim;\n-      float[] values = layers[node / numNodeOneRow];\n-      for (int a = 0; a < dim; a++) values[a + col] += inputUpdates[offset + a] / divider;\n+        Int2IntMap.Entry entry = it.next();\n+        int node = entry.getIntKey();\n+        int offset = entry.getIntValue() * dim;\n+        int divider = inputUpdateCounter.get(node);\n+        int col = (node % numNodeOneRow) * dim;\n+        float[] values = layers[node / numNodeOneRow];\n+        for (int a = 0; a < dim; a++)\n+                             values[a + col] += inputUpdates[offset + a] / divider;\n     }\n-  }\n-}\n+    }\n+\n+}\n\\ No newline at end of file\n",
            "diff_size": 80
        },
        {
            "tool": "styler_random",
            "errors": [
                {
                    "line": "15",
                    "column": "42",
                    "severity": "error",
                    "message": "'(' is preceded with whitespace.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.MethodParamPadCheck"
                }
            ],
            "diff": "diff --git a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/54/LINEFirstOrderModel.java b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/styler_random/54/LINEFirstOrderModel.java\nindex 4af089cbb9..081fa2b45b 100644\n--- a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/54/LINEFirstOrderModel.java\n+++ b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/styler_random/54/LINEFirstOrderModel.java\n@@ -11,103 +11,102 @@ import java.util.Random;\n \n public class LINEFirstOrderModel extends EmbeddingModel {\n \n-\n-  public LINEFirstOrderModel(int dim, int negative, int seed, int maxIndex, int numNodeOneRow, float[][] layers) {\n-    super(dim, negative, seed, maxIndex, numNodeOneRow, layers);\n-  }\n-\n-  @Override\n-  public float[] dot(ByteBuf edges) {\n-\n-    Random negativeSeed = new Random(seed);\n-    IntOpenHashSet numInputs = new IntOpenHashSet();\n-\n-    int batchSize = edges.readInt();\n-    float[] partialDots = new float[batchSize * (1 + negative)];\n-    int dotInc = 0;\n-    for (int position = 0; position < batchSize; position++) {\n-      int src = edges.readInt();\n-      int dst = edges.readInt();\n-      // Skip-Gram model\n-      float[] inputs = layers[src / numNodeOneRow];\n-      int l1 = (src % numNodeOneRow) * dim;\n-      numInputs.add(src);\n-\n-      // Negative sampling\n-      int target;\n-      for (int a = 0; a < negative + 1; a++) {\n-        if (a == 0) target = dst;\n-        else do {\n-          target = negativeSeed.nextInt(maxIndex);\n-        } while (target == src);\n-\n-        numInputs.add(target);\n-        float[] outputs = layers[target / numNodeOneRow];\n-        int l2 = (target % numNodeOneRow) * dim;\n-        float f = 0.0f;\n-        for (int b = 0; b < dim; b++) f += inputs[l1 + b] * outputs[l2 + b];\n-        partialDots[dotInc++] = f;\n-      }\n-    }\n-    this.numInputsToUpdate = numInputs.size();\n-    return partialDots;\n-  }\n-\n-  @Override\n-  public void adjust(ByteBuf dataBuf, int numInputs, int numOutputs) {\n-\n-    // used to accumulate the updates for input vectors\n-    float[] neu1e = new float[dim];\n-    float[] inputUpdates = new float[numInputs * dim];\n-    Int2IntOpenHashMap inputIndex = new Int2IntOpenHashMap();\n-    Int2IntOpenHashMap inputUpdateCounter = new Int2IntOpenHashMap();\n-    Random negativeSeed = new Random(seed);\n-    int batchSize = dataBuf.readInt();\n-\n-    for (int position = 0; position < batchSize; position++) {\n-      int src = dataBuf.readInt();\n-      int dst = dataBuf.readInt();\n-\n-      float[] inputs = layers[src / numNodeOneRow];\n-      int l1 = (src % numNodeOneRow) * dim;\n-\n-      Arrays.fill(neu1e, 0);\n-\n-      // Negative sampling\n-      int target;\n-      for (int d = 0; d < negative + 1; d++) {\n-        if (d == 0) target = dst;\n-        else do {\n-          target = negativeSeed.nextInt(maxIndex);\n-        } while (target == src);\n-\n-        float[] outputs = layers[target / numNodeOneRow];\n-        int l2 = (target % numNodeOneRow) * dim;\n-\n-        float g = dataBuf.readFloat();\n-\n-        // accumulate for the hidden layer\n-        for (int a = 0; a < dim; a++) neu1e[a] += g * outputs[a + l2];\n-        // update output layer\n-        merge(inputUpdates, inputIndex, target, inputs, g, l1);\n-        inputUpdateCounter.addTo(target, 1);\n-      }\n-\n-      // update the hidden layer\n-      merge(inputUpdates, inputIndex, src, neu1e, 1, 0);\n-      inputUpdateCounter.addTo(src, 1);\n-    }\n-\n-    // update input\n-    ObjectIterator<Int2IntMap.Entry> it = inputIndex.int2IntEntrySet().fastIterator();\n-    while (it.hasNext()) {\n-      Int2IntMap.Entry entry = it.next();\n-      int node = entry.getIntKey();\n-      int offset = entry.getIntValue() * dim;\n-      int divider = inputUpdateCounter.get(node);\n-      int col = (node % numNodeOneRow) * dim;\n-      float[] values = layers[node / numNodeOneRow];\n-      for (int a = 0; a < dim; a++) values[a + col] += inputUpdates[offset + a] / divider;\n-    }\n-  }\n+  public LINEFirstOrderModel(int dim, int negative, int seed, int maxIndex, int\n+  numNodeOneRow,float[ ] []layers){super (dim, negative, seed, maxIndex, numNodeOneRow, layers);\n+ }\n+\n+ @Override\n+ public float[] dot(ByteBuf edges) {\n+\n+   Random negativeSeed = new Random(seed);\n+   IntOpenHashSet numInputs = new IntOpenHashSet();\n+\n+   int batchSize = edges.readInt();\n+   float[] partialDots = new float[batchSize * (1 + negative)];\n+   int dotInc = 0;\n+   for (int position = 0; position < batchSize; position++) {\n+     int src = edges.readInt();\n+     int dst = edges.readInt();\n+     // Skip-Gram model\n+     float[] inputs = layers[src / numNodeOneRow];\n+     int l1 = (src % numNodeOneRow) * dim;\n+     numInputs.add(src);\n+\n+     // Negative sampling\n+     int target;\n+     for (int a = 0; a < negative + 1; a++) {\n+       if (a == 0) target = dst;\n+       else do {\n+         target = negativeSeed.nextInt(maxIndex);\n+       } while (target == src);\n+\n+       numInputs.add(target);\n+       float[] outputs = layers[target / numNodeOneRow];\n+       int l2 = (target % numNodeOneRow) * dim;\n+       float f = 0.0f;\n+       for (int b = 0; b < dim; b++) f += inputs[l1 + b] * outputs[l2 + b];\n+       partialDots[dotInc++] = f;\n+     }\n+   }\n+   this.numInputsToUpdate = numInputs.size();\n+   return partialDots;\n+ }\n+\n+ @Override\n+ public void adjust(ByteBuf dataBuf, int numInputs, int numOutputs) {\n+\n+   // used to accumulate the updates for input vectors\n+   float[] neu1e = new float[dim];\n+   float[] inputUpdates = new float[numInputs * dim];\n+   Int2IntOpenHashMap inputIndex = new Int2IntOpenHashMap();\n+   Int2IntOpenHashMap inputUpdateCounter = new Int2IntOpenHashMap();\n+   Random negativeSeed = new Random(seed);\n+   int batchSize = dataBuf.readInt();\n+\n+   for (int position = 0; position < batchSize; position++) {\n+     int src = dataBuf.readInt();\n+     int dst = dataBuf.readInt();\n+\n+     float[] inputs = layers[src / numNodeOneRow];\n+     int l1 = (src % numNodeOneRow) * dim;\n+\n+     Arrays.fill(neu1e, 0);\n+\n+     // Negative sampling\n+     int target;\n+     for (int d = 0; d < negative + 1; d++) {\n+       if (d == 0) target = dst;\n+       else do {\n+         target = negativeSeed.nextInt(maxIndex);\n+       } while (target == src);\n+\n+       float[] outputs = layers[target / numNodeOneRow];\n+       int l2 = (target % numNodeOneRow) * dim;\n+\n+       float g = dataBuf.readFloat();\n+\n+       // accumulate for the hidden layer\n+       for (int a = 0; a < dim; a++) neu1e[a] += g * outputs[a + l2];\n+       // update output layer\n+       merge(inputUpdates, inputIndex, target, inputs, g, l1);\n+       inputUpdateCounter.addTo(target, 1);\n+     }\n+\n+     // update the hidden layer\n+     merge(inputUpdates, inputIndex, src, neu1e, 1, 0);\n+     inputUpdateCounter.addTo(src, 1);\n+   }\n+\n+   // update input\n+   ObjectIterator<Int2IntMap.Entry> it = inputIndex.int2IntEntrySet().fastIterator();\n+   while (it.hasNext()) {\n+     Int2IntMap.Entry entry = it.next();\n+     int node = entry.getIntKey();\n+     int offset = entry.getIntValue() * dim;\n+     int divider = inputUpdateCounter.get(node);\n+     int col = (node % numNodeOneRow) * dim;\n+     float[] values = layers[node / numNodeOneRow];\n+     for (int a = 0; a < dim; a++) values[a + col] += inputUpdates[offset + a] / divider;\n+   }\n+ }\n }\n",
            "diff_size": 99
        },
        {
            "tool": "styler_three_grams",
            "errors": [
                {
                    "line": "12",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 170).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                }
            ],
            "diff": "diff --git a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/54/LINEFirstOrderModel.java b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/styler_three_grams/54/LINEFirstOrderModel.java\nindex 4af089cbb9..223cad9659 100644\n--- a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/54/LINEFirstOrderModel.java\n+++ b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/styler_three_grams/54/LINEFirstOrderModel.java\n@@ -9,105 +9,102 @@ import it.unimi.dsi.fastutil.objects.ObjectIterator;\n import java.util.Arrays;\n import java.util.Random;\n \n-public class LINEFirstOrderModel extends EmbeddingModel {\n-\n-\n-  public LINEFirstOrderModel(int dim, int negative, int seed, int maxIndex, int numNodeOneRow, float[][] layers) {\n-    super(dim, negative, seed, maxIndex, numNodeOneRow, layers);\n-  }\n+public class LINEFirstOrderModel extends EmbeddingModel { public LINEFirstOrderModel(int dim, int negative, int seed, int maxIndex, int numNodeOneRow, float[][] layers) {\n+  super(dim, negative, seed, maxIndex, numNodeOneRow, layers);\n+}\n \n-  @Override\n-  public float[] dot(ByteBuf edges) {\n-\n-    Random negativeSeed = new Random(seed);\n-    IntOpenHashSet numInputs = new IntOpenHashSet();\n-\n-    int batchSize = edges.readInt();\n-    float[] partialDots = new float[batchSize * (1 + negative)];\n-    int dotInc = 0;\n-    for (int position = 0; position < batchSize; position++) {\n-      int src = edges.readInt();\n-      int dst = edges.readInt();\n-      // Skip-Gram model\n-      float[] inputs = layers[src / numNodeOneRow];\n-      int l1 = (src % numNodeOneRow) * dim;\n-      numInputs.add(src);\n-\n-      // Negative sampling\n-      int target;\n-      for (int a = 0; a < negative + 1; a++) {\n-        if (a == 0) target = dst;\n-        else do {\n-          target = negativeSeed.nextInt(maxIndex);\n-        } while (target == src);\n-\n-        numInputs.add(target);\n-        float[] outputs = layers[target / numNodeOneRow];\n-        int l2 = (target % numNodeOneRow) * dim;\n-        float f = 0.0f;\n-        for (int b = 0; b < dim; b++) f += inputs[l1 + b] * outputs[l2 + b];\n-        partialDots[dotInc++] = f;\n-      }\n+@Override\n+public float[] dot(ByteBuf edges) {\n+\n+  Random negativeSeed = new Random(seed);\n+  IntOpenHashSet numInputs = new IntOpenHashSet();\n+\n+  int batchSize = edges.readInt();\n+  float[] partialDots = new float[batchSize * (1 + negative)];\n+  int dotInc = 0;\n+  for (int position = 0; position < batchSize; position++) {\n+    int src = edges.readInt();\n+    int dst = edges.readInt();\n+    // Skip-Gram model\n+    float[] inputs = layers[src / numNodeOneRow];\n+    int l1 = (src % numNodeOneRow) * dim;\n+    numInputs.add(src);\n+\n+    // Negative sampling\n+    int target;\n+    for (int a = 0; a < negative + 1; a++) {\n+      if (a == 0) target = dst;\n+      else do {\n+        target = negativeSeed.nextInt(maxIndex);\n+      } while (target == src);\n+\n+      numInputs.add(target);\n+      float[] outputs = layers[target / numNodeOneRow];\n+      int l2 = (target % numNodeOneRow) * dim;\n+      float f = 0.0f;\n+      for (int b = 0; b < dim; b++) f += inputs[l1 + b] * outputs[l2 + b];\n+      partialDots[dotInc++] = f;\n     }\n-    this.numInputsToUpdate = numInputs.size();\n-    return partialDots;\n   }\n+  this.numInputsToUpdate = numInputs.size();\n+  return partialDots;\n+}\n \n-  @Override\n-  public void adjust(ByteBuf dataBuf, int numInputs, int numOutputs) {\n-\n-    // used to accumulate the updates for input vectors\n-    float[] neu1e = new float[dim];\n-    float[] inputUpdates = new float[numInputs * dim];\n-    Int2IntOpenHashMap inputIndex = new Int2IntOpenHashMap();\n-    Int2IntOpenHashMap inputUpdateCounter = new Int2IntOpenHashMap();\n-    Random negativeSeed = new Random(seed);\n-    int batchSize = dataBuf.readInt();\n-\n-    for (int position = 0; position < batchSize; position++) {\n-      int src = dataBuf.readInt();\n-      int dst = dataBuf.readInt();\n-\n-      float[] inputs = layers[src / numNodeOneRow];\n-      int l1 = (src % numNodeOneRow) * dim;\n-\n-      Arrays.fill(neu1e, 0);\n-\n-      // Negative sampling\n-      int target;\n-      for (int d = 0; d < negative + 1; d++) {\n-        if (d == 0) target = dst;\n-        else do {\n-          target = negativeSeed.nextInt(maxIndex);\n-        } while (target == src);\n-\n-        float[] outputs = layers[target / numNodeOneRow];\n-        int l2 = (target % numNodeOneRow) * dim;\n-\n-        float g = dataBuf.readFloat();\n-\n-        // accumulate for the hidden layer\n-        for (int a = 0; a < dim; a++) neu1e[a] += g * outputs[a + l2];\n-        // update output layer\n-        merge(inputUpdates, inputIndex, target, inputs, g, l1);\n-        inputUpdateCounter.addTo(target, 1);\n-      }\n-\n-      // update the hidden layer\n-      merge(inputUpdates, inputIndex, src, neu1e, 1, 0);\n-      inputUpdateCounter.addTo(src, 1);\n-    }\n+@Override\n+public void adjust(ByteBuf dataBuf, int numInputs, int numOutputs) {\n+\n+  // used to accumulate the updates for input vectors\n+  float[] neu1e = new float[dim];\n+  float[] inputUpdates = new float[numInputs * dim];\n+  Int2IntOpenHashMap inputIndex = new Int2IntOpenHashMap();\n+  Int2IntOpenHashMap inputUpdateCounter = new Int2IntOpenHashMap();\n+  Random negativeSeed = new Random(seed);\n+  int batchSize = dataBuf.readInt();\n+\n+  for (int position = 0; position < batchSize; position++) {\n+    int src = dataBuf.readInt();\n+    int dst = dataBuf.readInt();\n+\n+    float[] inputs = layers[src / numNodeOneRow];\n+    int l1 = (src % numNodeOneRow) * dim;\n \n-    // update input\n-    ObjectIterator<Int2IntMap.Entry> it = inputIndex.int2IntEntrySet().fastIterator();\n-    while (it.hasNext()) {\n-      Int2IntMap.Entry entry = it.next();\n-      int node = entry.getIntKey();\n-      int offset = entry.getIntValue() * dim;\n-      int divider = inputUpdateCounter.get(node);\n-      int col = (node % numNodeOneRow) * dim;\n-      float[] values = layers[node / numNodeOneRow];\n-      for (int a = 0; a < dim; a++) values[a + col] += inputUpdates[offset + a] / divider;\n+    Arrays.fill(neu1e, 0);\n+\n+    // Negative sampling\n+    int target;\n+    for (int d = 0; d < negative + 1; d++) {\n+      if (d == 0) target = dst;\n+      else do {\n+        target = negativeSeed.nextInt(maxIndex);\n+      } while (target == src);\n+\n+      float[] outputs = layers[target / numNodeOneRow];\n+      int l2 = (target % numNodeOneRow) * dim;\n+\n+      float g = dataBuf.readFloat();\n+\n+      // accumulate for the hidden layer\n+      for (int a = 0; a < dim; a++) neu1e[a] += g * outputs[a + l2];\n+      // update output layer\n+      merge(inputUpdates, inputIndex, target, inputs, g, l1);\n+      inputUpdateCounter.addTo(target, 1);\n     }\n+\n+    // update the hidden layer\n+    merge(inputUpdates, inputIndex, src, neu1e, 1, 0);\n+    inputUpdateCounter.addTo(src, 1);\n+  }\n+\n+  // update input\n+  ObjectIterator<Int2IntMap.Entry> it = inputIndex.int2IntEntrySet().fastIterator();\n+  while (it.hasNext()) {\n+    Int2IntMap.Entry entry = it.next();\n+    int node = entry.getIntKey();\n+    int offset = entry.getIntValue() * dim;\n+    int divider = inputUpdateCounter.get(node);\n+    int col = (node % numNodeOneRow) * dim;\n+    float[] values = layers[node / numNodeOneRow];\n+    for (int a = 0; a < dim; a++) values[a + col] += inputUpdates[offset + a] / divider;\n   }\n }\n+}\n",
            "diff_size": 124
        }
    ]
}