{
    "error_id": "446",
    "information": {
        "errors": [
            {
                "line": "6",
                "severity": "error",
                "message": "Line is longer than 100 characters (found 102).",
                "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
            }
        ]
    },
    "source_code": " * Copyright (C) 2017-2018 THL A29 Limited, a Tencent company. All rights reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in\n * compliance with the License. You may obtain a copy of the License at\n *\n * https://opensource.org/licenses/Apache-2.0",
    "results": [
        {
            "tool": "styler",
            "errors": [
                {
                    "line": "6",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 102).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        },
        {
            "tool": "intellij",
            "errors": [
                {
                    "line": "6",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 102).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                }
            ],
            "diff": "diff --git a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/446/AdaGradUpdateFunc.java b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/intellij/446/AdaGradUpdateFunc.java\nindex a0204d4b30..2e7ade3965 100644\n--- a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/446/AdaGradUpdateFunc.java\n+++ b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/intellij/446/AdaGradUpdateFunc.java\n@@ -26,62 +26,62 @@ import org.apache.commons.logging.LogFactory;\n \n public class AdaGradUpdateFunc extends OptMMUpdateFunc {\n \n-  private static final Log LOG = LogFactory.getLog(AdaGradUpdateFunc.class);\n+    private static final Log LOG = LogFactory.getLog(AdaGradUpdateFunc.class);\n \n-  public AdaGradUpdateFunc() {\n-    super();\n-  }\n+    public AdaGradUpdateFunc() {\n+        super();\n+    }\n \n-  public AdaGradUpdateFunc(int matId, int factor, double epsilon, double beta, double lr,\n-      double regL1Param, double regL2Param, int epoch) {\n-    this(matId, factor, epsilon, beta, lr, regL1Param, regL2Param, epoch, 1);\n-  }\n+    public AdaGradUpdateFunc(int matId, int factor, double epsilon, double beta, double lr,\n+                             double regL1Param, double regL2Param, int epoch) {\n+        this(matId, factor, epsilon, beta, lr, regL1Param, regL2Param, epoch, 1);\n+    }\n \n-  public AdaGradUpdateFunc(int matId, int factor, double epsilon, double beta, double lr,\n-      double regL1Param, double regL2Param, int epoch, int batchSize) {\n-    super(matId, new int[]{factor},\n-        new double[]{epsilon, beta, lr, regL1Param, regL2Param, epoch, batchSize});\n-  }\n+    public AdaGradUpdateFunc(int matId, int factor, double epsilon, double beta, double lr,\n+                             double regL1Param, double regL2Param, int epoch, int batchSize) {\n+        super(matId, new int[] {factor},\n+                new double[] {epsilon, beta, lr, regL1Param, regL2Param, epoch, batchSize});\n+    }\n \n-  @Override\n-  public void update(ServerPartition partition, int factor, double[] scalars) {\n-    double epsilon = scalars[0];\n-    double beta = scalars[1];\n-    double lr = scalars[2];\n-    double l1RegParam = scalars[3];\n-    double l2RegParam = scalars[4];\n-    double epoch = (int) scalars[5];\n-    double batchSize = (int) scalars[6];\n+    @Override\n+    public void update(ServerPartition partition, int factor, double[] scalars) {\n+        double epsilon = scalars[0];\n+        double beta = scalars[1];\n+        double lr = scalars[2];\n+        double l1RegParam = scalars[3];\n+        double l2RegParam = scalars[4];\n+        double epoch = (int) scalars[5];\n+        double batchSize = (int) scalars[6];\n \n-    for (int f = 0; f < factor; f++) {\n-      ServerRow gradientServerRow = partition.getRow(f + 2 * factor);\n-      try {\n-        gradientServerRow.startWrite();\n-        Vector weight = partition.getRow(f).getSplit();\n-        Vector square = partition.getRow(f + factor).getSplit();\n-        Vector gradient = gradientServerRow.getSplit();\n+        for (int f = 0; f < factor; f++) {\n+            ServerRow gradientServerRow = partition.getRow(f + 2 * factor);\n+            try {\n+                gradientServerRow.startWrite();\n+                Vector weight = partition.getRow(f).getSplit();\n+                Vector square = partition.getRow(f + factor).getSplit();\n+                Vector gradient = gradientServerRow.getSplit();\n \n-        if (batchSize > 1) {\n-          gradient.idiv(batchSize);\n-        }\n+                if (batchSize > 1) {\n+                    gradient.idiv(batchSize);\n+                }\n \n-        OptFuncs.iexpsmoothing2(square, gradient, beta);\n-        if (l2RegParam != 0) {\n-          gradient.iaxpy(weight, l2RegParam);\n-        }\n+                OptFuncs.iexpsmoothing2(square, gradient, beta);\n+                if (l2RegParam != 0) {\n+                    gradient.iaxpy(weight, l2RegParam);\n+                }\n \n-        OptFuncs.iadagraddelta(gradient, square, l2RegParam, lr);\n-        weight.isub(gradient);\n+                OptFuncs.iadagraddelta(gradient, square, l2RegParam, lr);\n+                weight.isub(gradient);\n \n-        if (l1RegParam != 0) {\n-          OptFuncs.iadagradthredshold(weight, square, l1RegParam, l2RegParam, lr);\n-        }\n+                if (l1RegParam != 0) {\n+                    OptFuncs.iadagradthredshold(weight, square, l1RegParam, l2RegParam, lr);\n+                }\n \n-        gradient.clear();\n-      } finally {\n-        gradientServerRow.endWrite();\n-      }\n+                gradient.clear();\n+            } finally {\n+                gradientServerRow.endWrite();\n+            }\n \n+        }\n     }\n-  }\n }\n",
            "diff_size": 47
        },
        {
            "tool": "naturalize",
            "errors": [
                {
                    "line": "6",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 102).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "35",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 140).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "39",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 155).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "40",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 111).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                }
            ],
            "diff": "diff --git a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/446/AdaGradUpdateFunc.java b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/naturalize/446/AdaGradUpdateFunc.java\nindex a0204d4b30..392a5f8c32 100644\n--- a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/446/AdaGradUpdateFunc.java\n+++ b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/naturalize/446/AdaGradUpdateFunc.java\n@@ -32,15 +32,12 @@ public class AdaGradUpdateFunc extends OptMMUpdateFunc {\n     super();\n   }\n \n-  public AdaGradUpdateFunc(int matId, int factor, double epsilon, double beta, double lr,\n-      double regL1Param, double regL2Param, int epoch) {\n-    this(matId, factor, epsilon, beta, lr, regL1Param, regL2Param, epoch, 1);\n+  public AdaGradUpdateFunc(int matId, int factor, double epsilon, double beta, double lr, double regL1Param, double regL2Param, int epoch) {\n+  this(matId, factor, epsilon, beta, lr, regL1Param, regL2Param, epoch, 1);\n   }\n \n-  public AdaGradUpdateFunc(int matId, int factor, double epsilon, double beta, double lr,\n-      double regL1Param, double regL2Param, int epoch, int batchSize) {\n-    super(matId, new int[]{factor},\n-        new double[]{epsilon, beta, lr, regL1Param, regL2Param, epoch, batchSize});\n+  public AdaGradUpdateFunc(int matId, int factor, double epsilon, double beta, double lr, double regL1Param, double regL2Param, int epoch, int batchSize) {\n+    super(matId, new int[]{factor}, new double[]{epsilon, beta, lr, regL1Param, regL2Param, epoch, batchSize});\n   }\n \n   @Override\n@@ -84,4 +81,4 @@ public class AdaGradUpdateFunc extends OptMMUpdateFunc {\n \n     }\n   }\n-}\n+}\n\\ No newline at end of file\n",
            "diff_size": 8
        },
        {
            "tool": "codebuff",
            "errors": [
                {
                    "line": "6",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 102).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "52",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 111).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                }
            ],
            "diff": "diff --git a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/446/AdaGradUpdateFunc.java b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/codebuff/446/AdaGradUpdateFunc.java\nindex a0204d4b30..8d68fa9757 100644\n--- a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/446/AdaGradUpdateFunc.java\n+++ b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/codebuff/446/AdaGradUpdateFunc.java\n@@ -25,26 +25,35 @@ import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n \n public class AdaGradUpdateFunc extends OptMMUpdateFunc {\n+    private static final Log LOG = LogFactory.getLog(AdaGradUpdateFunc.class);\n \n-  private static final Log LOG = LogFactory.getLog(AdaGradUpdateFunc.class);\n-\n-  public AdaGradUpdateFunc() {\n+    public AdaGradUpdateFunc() {\n     super();\n-  }\n+    }\n \n-  public AdaGradUpdateFunc(int matId, int factor, double epsilon, double beta, double lr,\n-      double regL1Param, double regL2Param, int epoch) {\n+    public AdaGradUpdateFunc(int matId,\n+                             int factor,\n+                             double epsilon,\n+                             double beta,\n+                             double lr,\n+                             double regL1Param,\n+                             double regL2Param, int epoch) {\n     this(matId, factor, epsilon, beta, lr, regL1Param, regL2Param, epoch, 1);\n-  }\n+    }\n \n-  public AdaGradUpdateFunc(int matId, int factor, double epsilon, double beta, double lr,\n-      double regL1Param, double regL2Param, int epoch, int batchSize) {\n-    super(matId, new int[]{factor},\n-        new double[]{epsilon, beta, lr, regL1Param, regL2Param, epoch, batchSize});\n-  }\n+    public AdaGradUpdateFunc(int matId,\n+                             int factor,\n+                             double epsilon,\n+                             double beta,\n+                             double lr,\n+                             double regL1Param,\n+                             double regL2Param,\n+                             int epoch, int batchSize) {\n+    super(matId, new int[]{factor}, new double[]{epsilon, beta, lr, regL1Param, regL2Param, epoch, batchSize});\n+    }\n \n-  @Override\n-  public void update(ServerPartition partition, int factor, double[] scalars) {\n+    @Override\n+    public void update(ServerPartition partition, int factor, double[] scalars) {\n     double epsilon = scalars[0];\n     double beta = scalars[1];\n     double lr = scalars[2];\n@@ -52,36 +61,30 @@ public class AdaGradUpdateFunc extends OptMMUpdateFunc {\n     double l2RegParam = scalars[4];\n     double epoch = (int) scalars[5];\n     double batchSize = (int) scalars[6];\n-\n     for (int f = 0; f < factor; f++) {\n-      ServerRow gradientServerRow = partition.getRow(f + 2 * factor);\n-      try {\n-        gradientServerRow.startWrite();\n-        Vector weight = partition.getRow(f).getSplit();\n-        Vector square = partition.getRow(f + factor).getSplit();\n-        Vector gradient = gradientServerRow.getSplit();\n-\n-        if (batchSize > 1) {\n-          gradient.idiv(batchSize);\n-        }\n-\n-        OptFuncs.iexpsmoothing2(square, gradient, beta);\n-        if (l2RegParam != 0) {\n-          gradient.iaxpy(weight, l2RegParam);\n+        ServerRow gradientServerRow = partition.getRow(f + 2 * factor);\n+        try {\n+                                         gradientServerRow.startWrite();\n+                                         Vector weight = partition.getRow(f).getSplit();\n+                                         Vector square = partition.getRow(f + factor).getSplit();\n+                                         Vector gradient = gradientServerRow.getSplit();\n+                                         if (batchSize > 1) {\n+            gradient.idiv(batchSize);\n+                                         }\n+                                         OptFuncs.iexpsmoothing2(square, gradient, beta);\n+                                         if (l2RegParam != 0) {\n+            gradient.iaxpy(weight, l2RegParam);\n+                                         }\n+                                         OptFuncs.iadagraddelta(gradient, square, l2RegParam, lr);\n+                                         weight.isub(gradient);\n+                                         if (l1RegParam != 0) {\n+            OptFuncs.iadagradthredshold(weight, square, l1RegParam, l2RegParam, lr);\n+                                         }\n+                                         gradient.clear();\n+        } finally {\n+          gradientServerRow.endWrite();\n         }\n-\n-        OptFuncs.iadagraddelta(gradient, square, l2RegParam, lr);\n-        weight.isub(gradient);\n-\n-        if (l1RegParam != 0) {\n-          OptFuncs.iadagradthredshold(weight, square, l1RegParam, l2RegParam, lr);\n-        }\n-\n-        gradient.clear();\n-      } finally {\n-        gradientServerRow.endWrite();\n-      }\n-\n     }\n-  }\n-}\n+    }\n+\n+}\n\\ No newline at end of file\n",
            "diff_size": 63
        },
        {
            "tool": "styler_random",
            "errors": [
                {
                    "line": "6",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 102).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        },
        {
            "tool": "styler_three_grams",
            "errors": [
                {
                    "line": "6",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 102).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        }
    ]
}