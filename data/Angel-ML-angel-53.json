{
    "error_id": "53",
    "information": {
        "errors": [
            {
                "line": "15",
                "severity": "error",
                "message": "Line is longer than 100 characters (found 115).",
                "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
            }
        ]
    },
    "source_code": "\n\n  public LINESecondOrderModel(int dim, int negative, int seed, int maxIndex, int numNodeOneRow, float[][] layers) {\n    super(dim, negative, seed, maxIndex, numNodeOneRow, layers);\n  }\n",
    "results": [
        {
            "tool": "styler",
            "errors": [
                {
                    "line": "15",
                    "column": "42",
                    "severity": "error",
                    "message": "'(' is preceded with whitespace.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.MethodParamPadCheck"
                }
            ],
            "diff": "diff --git a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/53/LINESecondOrderModel.java b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/styler/53/LINESecondOrderModel.java\nindex 664a84e90f..2b760da14d 100644\n--- a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/53/LINESecondOrderModel.java\n+++ b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/styler/53/LINESecondOrderModel.java\n@@ -11,124 +11,123 @@ import java.util.Random;\n \n public class LINESecondOrderModel extends EmbeddingModel {\n \n-\n-  public LINESecondOrderModel(int dim, int negative, int seed, int maxIndex, int numNodeOneRow, float[][] layers) {\n-    super(dim, negative, seed, maxIndex, numNodeOneRow, layers);\n-  }\n-\n-  @Override\n-  public float[] dot(ByteBuf edges) {\n-\n-    Random negativeSeed = new Random(seed);\n-    IntOpenHashSet numInputs = new IntOpenHashSet();\n-    IntOpenHashSet numOutputs = new IntOpenHashSet();\n-\n-    int batchSize = edges.readInt();\n-    float[] partialDots = new float[batchSize * (1 + negative)];\n-    int dotInc = 0;\n-    for (int position = 0; position < batchSize; position++) {\n-      int src = edges.readInt();\n-      // Skip-Gram model\n-      float[] inputs = layers[src / numNodeOneRow];\n-      int l1 = (src % numNodeOneRow) * dim * 2;\n-      numInputs.add(src);\n-\n-      // Negative sampling\n-      int target;\n-      for (int a = 0; a < negative + 1; a++) {\n-        if (a == 0) target = edges.readInt();\n-        else do {\n-          target = negativeSeed.nextInt(maxIndex);\n-        } while (target == src);\n-\n-        numOutputs.add(target);\n-        float[] outputs = layers[target / numNodeOneRow];\n-        int l2 = (target % numNodeOneRow) * dim * 2 + dim;\n-        float f = 0.0f;\n-        for (int b = 0; b < dim; b++) f += inputs[l1 + b] * outputs[l2 + b];\n-        partialDots[dotInc++] = f;\n-      }\n-    }\n-\n-    this.numInputsToUpdate = numInputs.size();\n-    this.numOutputsToUpdate = numOutputs.size();\n-\n-    return partialDots;\n-  }\n-\n-  @Override\n-  public void adjust(ByteBuf dataBuf, int numInputs, int numOutputs) {\n-\n-    // used to accumulate the updates for input vectors\n-    float[] neu1e = new float[dim];\n-\n-    float[] inputUpdates = new float[numInputs * dim];\n-    float[] outputUpdates = new float[numOutputs * dim];\n-\n-    Int2IntOpenHashMap inputIndex = new Int2IntOpenHashMap();\n-    Int2IntOpenHashMap outputIndex = new Int2IntOpenHashMap();\n-\n-    Int2IntOpenHashMap inputUpdateCounter = new Int2IntOpenHashMap();\n-    Int2IntOpenHashMap outputUpdateCounter = new Int2IntOpenHashMap();\n-\n-    Random negativeSeed = new Random(seed);\n-    int batchSize = dataBuf.readInt();\n-\n-    for (int position = 0; position < batchSize; position++) {\n-      int src = dataBuf.readInt();\n-\n-      float[] inputs = layers[src / numNodeOneRow];\n-      int l1 = (src % numNodeOneRow) * dim * 2;\n-\n-      Arrays.fill(neu1e, 0);\n-\n-      // Negative sampling\n-      int target;\n-      for (int d = 0; d < negative + 1; d++) {\n-        if (d == 0) target = dataBuf.readInt();\n-        else do {\n-          target = negativeSeed.nextInt(maxIndex);\n-        } while (target == src);\n-\n-        float[] outputs = layers[target / numNodeOneRow];\n-        int l2 = (target % numNodeOneRow) * dim * 2 + dim;\n-\n-        float g = dataBuf.readFloat();\n-\n-        // accumulate for the hidden layer\n-        for (int a = 0; a < dim; a++) neu1e[a] += g * outputs[a + l2];\n-        // update output layer\n-        merge(outputUpdates, outputIndex, target, inputs, g, l1);\n-        outputUpdateCounter.addTo(target, 1);\n-      }\n-\n-      // update the hidden layer\n-      merge(inputUpdates, inputIndex, src, neu1e, 1, 0);\n-      inputUpdateCounter.addTo(src, 1);\n-    }\n-\n-    // update input\n-    ObjectIterator<Int2IntMap.Entry> it = inputIndex.int2IntEntrySet().fastIterator();\n-    while (it.hasNext()) {\n-      Int2IntMap.Entry entry = it.next();\n-      int node = entry.getIntKey();\n-      int offset = entry.getIntValue() * dim;\n-      int divider = inputUpdateCounter.get(node);\n-      int col = (node % numNodeOneRow) * dim * 2;\n-      float[] values = layers[node / numNodeOneRow];\n-      for (int a = 0; a < dim; a++) values[a + col] += inputUpdates[offset + a] / divider;\n-    }\n-\n-    // update output\n-    it = outputIndex.int2IntEntrySet().fastIterator();\n-    while (it.hasNext()) {\n-      Int2IntMap.Entry entry = it.next();\n-      int node = entry.getIntKey();\n-      int offset = entry.getIntValue() * dim;\n-      int col = (node % numNodeOneRow) * dim * 2 + dim;\n-      float[] values = layers[node / numNodeOneRow];\n-      int divider = outputUpdateCounter.get(node);\n-      for (int a = 0; a < dim; a++) values[a + col] += outputUpdates[offset + a] / divider;\n-    }\n-  }\n+  public LINESecondOrderModel(int dim, int negative, int seed, int maxIndex, int\n+  numNodeOneRow,float[ ] []layers){super (dim, negative, seed, maxIndex, numNodeOneRow, layers);\n+ }\n+\n+ @Override\n+ public float[] dot(ByteBuf edges) {\n+\n+   Random negativeSeed = new Random(seed);\n+   IntOpenHashSet numInputs = new IntOpenHashSet();\n+   IntOpenHashSet numOutputs = new IntOpenHashSet();\n+\n+   int batchSize = edges.readInt();\n+   float[] partialDots = new float[batchSize * (1 + negative)];\n+   int dotInc = 0;\n+   for (int position = 0; position < batchSize; position++) {\n+     int src = edges.readInt();\n+     // Skip-Gram model\n+     float[] inputs = layers[src / numNodeOneRow];\n+     int l1 = (src % numNodeOneRow) * dim * 2;\n+     numInputs.add(src);\n+\n+     // Negative sampling\n+     int target;\n+     for (int a = 0; a < negative + 1; a++) {\n+       if (a == 0) target = edges.readInt();\n+       else do {\n+         target = negativeSeed.nextInt(maxIndex);\n+       } while (target == src);\n+\n+       numOutputs.add(target);\n+       float[] outputs = layers[target / numNodeOneRow];\n+       int l2 = (target % numNodeOneRow) * dim * 2 + dim;\n+       float f = 0.0f;\n+       for (int b = 0; b < dim; b++) f += inputs[l1 + b] * outputs[l2 + b];\n+       partialDots[dotInc++] = f;\n+     }\n+   }\n+\n+   this.numInputsToUpdate = numInputs.size();\n+   this.numOutputsToUpdate = numOutputs.size();\n+\n+   return partialDots;\n+ }\n+\n+ @Override\n+ public void adjust(ByteBuf dataBuf, int numInputs, int numOutputs) {\n+\n+   // used to accumulate the updates for input vectors\n+   float[] neu1e = new float[dim];\n+\n+   float[] inputUpdates = new float[numInputs * dim];\n+   float[] outputUpdates = new float[numOutputs * dim];\n+\n+   Int2IntOpenHashMap inputIndex = new Int2IntOpenHashMap();\n+   Int2IntOpenHashMap outputIndex = new Int2IntOpenHashMap();\n+\n+   Int2IntOpenHashMap inputUpdateCounter = new Int2IntOpenHashMap();\n+   Int2IntOpenHashMap outputUpdateCounter = new Int2IntOpenHashMap();\n+\n+   Random negativeSeed = new Random(seed);\n+   int batchSize = dataBuf.readInt();\n+\n+   for (int position = 0; position < batchSize; position++) {\n+     int src = dataBuf.readInt();\n+\n+     float[] inputs = layers[src / numNodeOneRow];\n+     int l1 = (src % numNodeOneRow) * dim * 2;\n+\n+     Arrays.fill(neu1e, 0);\n+\n+     // Negative sampling\n+     int target;\n+     for (int d = 0; d < negative + 1; d++) {\n+       if (d == 0) target = dataBuf.readInt();\n+       else do {\n+         target = negativeSeed.nextInt(maxIndex);\n+       } while (target == src);\n+\n+       float[] outputs = layers[target / numNodeOneRow];\n+       int l2 = (target % numNodeOneRow) * dim * 2 + dim;\n+\n+       float g = dataBuf.readFloat();\n+\n+       // accumulate for the hidden layer\n+       for (int a = 0; a < dim; a++) neu1e[a] += g * outputs[a + l2];\n+       // update output layer\n+       merge(outputUpdates, outputIndex, target, inputs, g, l1);\n+       outputUpdateCounter.addTo(target, 1);\n+     }\n+\n+     // update the hidden layer\n+     merge(inputUpdates, inputIndex, src, neu1e, 1, 0);\n+     inputUpdateCounter.addTo(src, 1);\n+   }\n+\n+   // update input\n+   ObjectIterator<Int2IntMap.Entry> it = inputIndex.int2IntEntrySet().fastIterator();\n+   while (it.hasNext()) {\n+     Int2IntMap.Entry entry = it.next();\n+     int node = entry.getIntKey();\n+     int offset = entry.getIntValue() * dim;\n+     int divider = inputUpdateCounter.get(node);\n+     int col = (node % numNodeOneRow) * dim * 2;\n+     float[] values = layers[node / numNodeOneRow];\n+     for (int a = 0; a < dim; a++) values[a + col] += inputUpdates[offset + a] / divider;\n+   }\n+\n+   // update output\n+   it = outputIndex.int2IntEntrySet().fastIterator();\n+   while (it.hasNext()) {\n+     Int2IntMap.Entry entry = it.next();\n+     int node = entry.getIntKey();\n+     int offset = entry.getIntValue() * dim;\n+     int col = (node % numNodeOneRow) * dim * 2 + dim;\n+     float[] values = layers[node / numNodeOneRow];\n+     int divider = outputUpdateCounter.get(node);\n+     for (int a = 0; a < dim; a++) values[a + col] += outputUpdates[offset + a] / divider;\n+   }\n+ }\n }\n",
            "diff_size": 120
        },
        {
            "tool": "intellij",
            "errors": [],
            "diff": "diff --git a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/53/LINESecondOrderModel.java b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/intellij/53/LINESecondOrderModel.java\nindex 664a84e90f..7fb2c5112c 100644\n--- a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/53/LINESecondOrderModel.java\n+++ b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/intellij/53/LINESecondOrderModel.java\n@@ -12,123 +12,130 @@ import java.util.Random;\n public class LINESecondOrderModel extends EmbeddingModel {\n \n \n-  public LINESecondOrderModel(int dim, int negative, int seed, int maxIndex, int numNodeOneRow, float[][] layers) {\n-    super(dim, negative, seed, maxIndex, numNodeOneRow, layers);\n-  }\n-\n-  @Override\n-  public float[] dot(ByteBuf edges) {\n-\n-    Random negativeSeed = new Random(seed);\n-    IntOpenHashSet numInputs = new IntOpenHashSet();\n-    IntOpenHashSet numOutputs = new IntOpenHashSet();\n-\n-    int batchSize = edges.readInt();\n-    float[] partialDots = new float[batchSize * (1 + negative)];\n-    int dotInc = 0;\n-    for (int position = 0; position < batchSize; position++) {\n-      int src = edges.readInt();\n-      // Skip-Gram model\n-      float[] inputs = layers[src / numNodeOneRow];\n-      int l1 = (src % numNodeOneRow) * dim * 2;\n-      numInputs.add(src);\n-\n-      // Negative sampling\n-      int target;\n-      for (int a = 0; a < negative + 1; a++) {\n-        if (a == 0) target = edges.readInt();\n-        else do {\n-          target = negativeSeed.nextInt(maxIndex);\n-        } while (target == src);\n-\n-        numOutputs.add(target);\n-        float[] outputs = layers[target / numNodeOneRow];\n-        int l2 = (target % numNodeOneRow) * dim * 2 + dim;\n-        float f = 0.0f;\n-        for (int b = 0; b < dim; b++) f += inputs[l1 + b] * outputs[l2 + b];\n-        partialDots[dotInc++] = f;\n-      }\n+    public LINESecondOrderModel(int dim, int negative, int seed, int maxIndex, int numNodeOneRow,\n+                                float[][] layers) {\n+        super(dim, negative, seed, maxIndex, numNodeOneRow, layers);\n     }\n \n-    this.numInputsToUpdate = numInputs.size();\n-    this.numOutputsToUpdate = numOutputs.size();\n-\n-    return partialDots;\n-  }\n-\n-  @Override\n-  public void adjust(ByteBuf dataBuf, int numInputs, int numOutputs) {\n-\n-    // used to accumulate the updates for input vectors\n-    float[] neu1e = new float[dim];\n-\n-    float[] inputUpdates = new float[numInputs * dim];\n-    float[] outputUpdates = new float[numOutputs * dim];\n-\n-    Int2IntOpenHashMap inputIndex = new Int2IntOpenHashMap();\n-    Int2IntOpenHashMap outputIndex = new Int2IntOpenHashMap();\n-\n-    Int2IntOpenHashMap inputUpdateCounter = new Int2IntOpenHashMap();\n-    Int2IntOpenHashMap outputUpdateCounter = new Int2IntOpenHashMap();\n-\n-    Random negativeSeed = new Random(seed);\n-    int batchSize = dataBuf.readInt();\n-\n-    for (int position = 0; position < batchSize; position++) {\n-      int src = dataBuf.readInt();\n-\n-      float[] inputs = layers[src / numNodeOneRow];\n-      int l1 = (src % numNodeOneRow) * dim * 2;\n-\n-      Arrays.fill(neu1e, 0);\n-\n-      // Negative sampling\n-      int target;\n-      for (int d = 0; d < negative + 1; d++) {\n-        if (d == 0) target = dataBuf.readInt();\n-        else do {\n-          target = negativeSeed.nextInt(maxIndex);\n-        } while (target == src);\n-\n-        float[] outputs = layers[target / numNodeOneRow];\n-        int l2 = (target % numNodeOneRow) * dim * 2 + dim;\n-\n-        float g = dataBuf.readFloat();\n-\n-        // accumulate for the hidden layer\n-        for (int a = 0; a < dim; a++) neu1e[a] += g * outputs[a + l2];\n-        // update output layer\n-        merge(outputUpdates, outputIndex, target, inputs, g, l1);\n-        outputUpdateCounter.addTo(target, 1);\n-      }\n-\n-      // update the hidden layer\n-      merge(inputUpdates, inputIndex, src, neu1e, 1, 0);\n-      inputUpdateCounter.addTo(src, 1);\n-    }\n-\n-    // update input\n-    ObjectIterator<Int2IntMap.Entry> it = inputIndex.int2IntEntrySet().fastIterator();\n-    while (it.hasNext()) {\n-      Int2IntMap.Entry entry = it.next();\n-      int node = entry.getIntKey();\n-      int offset = entry.getIntValue() * dim;\n-      int divider = inputUpdateCounter.get(node);\n-      int col = (node % numNodeOneRow) * dim * 2;\n-      float[] values = layers[node / numNodeOneRow];\n-      for (int a = 0; a < dim; a++) values[a + col] += inputUpdates[offset + a] / divider;\n+    @Override\n+    public float[] dot(ByteBuf edges) {\n+\n+        Random negativeSeed = new Random(seed);\n+        IntOpenHashSet numInputs = new IntOpenHashSet();\n+        IntOpenHashSet numOutputs = new IntOpenHashSet();\n+\n+        int batchSize = edges.readInt();\n+        float[] partialDots = new float[batchSize * (1 + negative)];\n+        int dotInc = 0;\n+        for (int position = 0; position < batchSize; position++) {\n+            int src = edges.readInt();\n+            // Skip-Gram model\n+            float[] inputs = layers[src / numNodeOneRow];\n+            int l1 = (src % numNodeOneRow) * dim * 2;\n+            numInputs.add(src);\n+\n+            // Negative sampling\n+            int target;\n+            for (int a = 0; a < negative + 1; a++) {\n+                if (a == 0) {\n+                    target = edges.readInt();\n+                } else {\n+                    do {\n+                        target = negativeSeed.nextInt(maxIndex);\n+                    } while (target == src);\n+                }\n+\n+                numOutputs.add(target);\n+                float[] outputs = layers[target / numNodeOneRow];\n+                int l2 = (target % numNodeOneRow) * dim * 2 + dim;\n+                float f = 0.0f;\n+                for (int b = 0; b < dim; b++) f += inputs[l1 + b] * outputs[l2 + b];\n+                partialDots[dotInc++] = f;\n+            }\n+        }\n+\n+        this.numInputsToUpdate = numInputs.size();\n+        this.numOutputsToUpdate = numOutputs.size();\n+\n+        return partialDots;\n     }\n \n-    // update output\n-    it = outputIndex.int2IntEntrySet().fastIterator();\n-    while (it.hasNext()) {\n-      Int2IntMap.Entry entry = it.next();\n-      int node = entry.getIntKey();\n-      int offset = entry.getIntValue() * dim;\n-      int col = (node % numNodeOneRow) * dim * 2 + dim;\n-      float[] values = layers[node / numNodeOneRow];\n-      int divider = outputUpdateCounter.get(node);\n-      for (int a = 0; a < dim; a++) values[a + col] += outputUpdates[offset + a] / divider;\n+    @Override\n+    public void adjust(ByteBuf dataBuf, int numInputs, int numOutputs) {\n+\n+        // used to accumulate the updates for input vectors\n+        float[] neu1e = new float[dim];\n+\n+        float[] inputUpdates = new float[numInputs * dim];\n+        float[] outputUpdates = new float[numOutputs * dim];\n+\n+        Int2IntOpenHashMap inputIndex = new Int2IntOpenHashMap();\n+        Int2IntOpenHashMap outputIndex = new Int2IntOpenHashMap();\n+\n+        Int2IntOpenHashMap inputUpdateCounter = new Int2IntOpenHashMap();\n+        Int2IntOpenHashMap outputUpdateCounter = new Int2IntOpenHashMap();\n+\n+        Random negativeSeed = new Random(seed);\n+        int batchSize = dataBuf.readInt();\n+\n+        for (int position = 0; position < batchSize; position++) {\n+            int src = dataBuf.readInt();\n+\n+            float[] inputs = layers[src / numNodeOneRow];\n+            int l1 = (src % numNodeOneRow) * dim * 2;\n+\n+            Arrays.fill(neu1e, 0);\n+\n+            // Negative sampling\n+            int target;\n+            for (int d = 0; d < negative + 1; d++) {\n+                if (d == 0) {\n+                    target = dataBuf.readInt();\n+                } else {\n+                    do {\n+                        target = negativeSeed.nextInt(maxIndex);\n+                    } while (target == src);\n+                }\n+\n+                float[] outputs = layers[target / numNodeOneRow];\n+                int l2 = (target % numNodeOneRow) * dim * 2 + dim;\n+\n+                float g = dataBuf.readFloat();\n+\n+                // accumulate for the hidden layer\n+                for (int a = 0; a < dim; a++) neu1e[a] += g * outputs[a + l2];\n+                // update output layer\n+                merge(outputUpdates, outputIndex, target, inputs, g, l1);\n+                outputUpdateCounter.addTo(target, 1);\n+            }\n+\n+            // update the hidden layer\n+            merge(inputUpdates, inputIndex, src, neu1e, 1, 0);\n+            inputUpdateCounter.addTo(src, 1);\n+        }\n+\n+        // update input\n+        ObjectIterator<Int2IntMap.Entry> it = inputIndex.int2IntEntrySet().fastIterator();\n+        while (it.hasNext()) {\n+            Int2IntMap.Entry entry = it.next();\n+            int node = entry.getIntKey();\n+            int offset = entry.getIntValue() * dim;\n+            int divider = inputUpdateCounter.get(node);\n+            int col = (node % numNodeOneRow) * dim * 2;\n+            float[] values = layers[node / numNodeOneRow];\n+            for (int a = 0; a < dim; a++) values[a + col] += inputUpdates[offset + a] / divider;\n+        }\n+\n+        // update output\n+        it = outputIndex.int2IntEntrySet().fastIterator();\n+        while (it.hasNext()) {\n+            Int2IntMap.Entry entry = it.next();\n+            int node = entry.getIntKey();\n+            int offset = entry.getIntValue() * dim;\n+            int col = (node % numNodeOneRow) * dim * 2 + dim;\n+            float[] values = layers[node / numNodeOneRow];\n+            int divider = outputUpdateCounter.get(node);\n+            for (int a = 0; a < dim; a++) values[a + col] += outputUpdates[offset + a] / divider;\n+        }\n     }\n-  }\n }\n",
            "diff_size": 181
        },
        {
            "tool": "naturalize",
            "errors": [
                {
                    "line": "14",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 115).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                }
            ],
            "diff": "diff --git a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/53/LINESecondOrderModel.java b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/naturalize/53/LINESecondOrderModel.java\nindex 664a84e90f..dfeffae359 100644\n--- a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/53/LINESecondOrderModel.java\n+++ b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/naturalize/53/LINESecondOrderModel.java\n@@ -11,7 +11,6 @@ import java.util.Random;\n \n public class LINESecondOrderModel extends EmbeddingModel {\n \n-\n   public LINESecondOrderModel(int dim, int negative, int seed, int maxIndex, int numNodeOneRow, float[][] layers) {\n     super(dim, negative, seed, maxIndex, numNodeOneRow, layers);\n   }\n@@ -45,7 +44,8 @@ public class LINESecondOrderModel extends EmbeddingModel {\n         float[] outputs = layers[target / numNodeOneRow];\n         int l2 = (target % numNodeOneRow) * dim * 2 + dim;\n         float f = 0.0f;\n-        for (int b = 0; b < dim; b++) f += inputs[l1 + b] * outputs[l2 + b];\n+        for (int b = 0; b < dim; b++)\n+  f += inputs[l1 + b] * outputs[l2 + b];\n         partialDots[dotInc++] = f;\n       }\n     }\n@@ -61,8 +61,7 @@ public class LINESecondOrderModel extends EmbeddingModel {\n \n     // used to accumulate the updates for input vectors\n     float[] neu1e = new float[dim];\n-\n-    float[] inputUpdates = new float[numInputs * dim];\n+float[] inputUpdates = new float[numInputs * dim];\n     float[] outputUpdates = new float[numOutputs * dim];\n \n     Int2IntOpenHashMap inputIndex = new Int2IntOpenHashMap();\n@@ -131,4 +130,4 @@ public class LINESecondOrderModel extends EmbeddingModel {\n       for (int a = 0; a < dim; a++) values[a + col] += outputUpdates[offset + a] / divider;\n     }\n   }\n-}\n+}\n\\ No newline at end of file\n",
            "diff_size": 6
        },
        {
            "tool": "codebuff",
            "errors": [
                {
                    "line": "13",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 117).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "27",
                    "severity": "error",
                    "message": "Comment has incorrect indentation level 6, expected is 8, indentation should be the same level as line 28.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.CommentsIndentationCheck"
                },
                {
                    "line": "32",
                    "severity": "error",
                    "message": "Comment has incorrect indentation level 6, expected is 8, indentation should be the same level as line 34.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.CommentsIndentationCheck"
                },
                {
                    "line": "36",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 102).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "40",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 101).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "42",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 114).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "43",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 115).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "77",
                    "severity": "error",
                    "message": "Comment has incorrect indentation level 6, expected is 8, indentation should be the same level as line 79.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.CommentsIndentationCheck"
                },
                {
                    "line": "81",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 104).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "85",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 101).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "86",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 114).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "87",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 115).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "90",
                    "severity": "error",
                    "message": "Comment has incorrect indentation level 8, expected is 65, indentation should be the same level as line 91.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.CommentsIndentationCheck"
                },
                {
                    "line": "93",
                    "severity": "error",
                    "message": "Comment has incorrect indentation level 8, expected is 65, indentation should be the same level as line 94.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.CommentsIndentationCheck"
                },
                {
                    "line": "94",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 122).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "95",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 102).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "98",
                    "severity": "error",
                    "message": "Comment has incorrect indentation level 6, expected is 8, indentation should be the same level as line 99.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.CommentsIndentationCheck"
                }
            ],
            "diff": "diff --git a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/53/LINESecondOrderModel.java b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/codebuff/53/LINESecondOrderModel.java\nindex 664a84e90f..6d67d45c61 100644\n--- a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/53/LINESecondOrderModel.java\n+++ b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/codebuff/53/LINESecondOrderModel.java\n@@ -5,130 +5,128 @@ import it.unimi.dsi.fastutil.ints.Int2IntMap;\n import it.unimi.dsi.fastutil.ints.Int2IntOpenHashMap;\n import it.unimi.dsi.fastutil.ints.IntOpenHashSet;\n import it.unimi.dsi.fastutil.objects.ObjectIterator;\n-\n import java.util.Arrays;\n import java.util.Random;\n \n public class LINESecondOrderModel extends EmbeddingModel {\n \n-\n-  public LINESecondOrderModel(int dim, int negative, int seed, int maxIndex, int numNodeOneRow, float[][] layers) {\n+    public LINESecondOrderModel(int dim, int negative, int seed, int maxIndex, int numNodeOneRow, float[][] layers) {\n     super(dim, negative, seed, maxIndex, numNodeOneRow, layers);\n-  }\n-\n-  @Override\n-  public float[] dot(ByteBuf edges) {\n+    }\n \n+    @Override\n+    public float[] dot(ByteBuf edges) {\n     Random negativeSeed = new Random(seed);\n     IntOpenHashSet numInputs = new IntOpenHashSet();\n     IntOpenHashSet numOutputs = new IntOpenHashSet();\n-\n     int batchSize = edges.readInt();\n     float[] partialDots = new float[batchSize * (1 + negative)];\n     int dotInc = 0;\n     for (int position = 0; position < batchSize; position++) {\n-      int src = edges.readInt();\n+        int src = edges.readInt();\n       // Skip-Gram model\n-      float[] inputs = layers[src / numNodeOneRow];\n-      int l1 = (src % numNodeOneRow) * dim * 2;\n-      numInputs.add(src);\n+        float[] inputs = layers[src / numNodeOneRow];\n+        int l1 = (src % numNodeOneRow) * dim * 2;\n+        numInputs.add(src);\n \n       // Negative sampling\n-      int target;\n-      for (int a = 0; a < negative + 1; a++) {\n-        if (a == 0) target = edges.readInt();\n-        else do {\n-          target = negativeSeed.nextInt(maxIndex);\n-        } while (target == src);\n-\n-        numOutputs.add(target);\n-        float[] outputs = layers[target / numNodeOneRow];\n-        int l2 = (target % numNodeOneRow) * dim * 2 + dim;\n-        float f = 0.0f;\n-        for (int b = 0; b < dim; b++) f += inputs[l1 + b] * outputs[l2 + b];\n-        partialDots[dotInc++] = f;\n-      }\n+\n+        int target;\n+        for (int a = 0; a < negative + 1; a++) {\n+                                                                 if (a == 0) target = edges.readInt();\n+                                                                 else\n+                                                                             do {\n+                                                   target = negativeSeed.nextInt(maxIndex);\n+                                                                             } while (target == src);\n+                                                                 numOutputs.add(target);\n+                                                                 float[] outputs = layers[target / numNodeOneRow];\n+                                                                 int l2 = (target % numNodeOneRow) * dim * 2 + dim;\n+                                                                 float f = 0.0f;\n+                                                                 for (int b = 0; b < dim; b++)\n+                                                   f += inputs[l1 + b] * outputs[l2 + b];\n+                                                                 partialDots[dotInc++] = f;\n+        }\n     }\n \n+\n+\n     this.numInputsToUpdate = numInputs.size();\n     this.numOutputsToUpdate = numOutputs.size();\n-\n     return partialDots;\n-  }\n+    }\n \n-  @Override\n-  public void adjust(ByteBuf dataBuf, int numInputs, int numOutputs) {\n+    @Override\n+    public void adjust(ByteBuf dataBuf, int numInputs, int numOutputs) {\n \n     // used to accumulate the updates for input vectors\n     float[] neu1e = new float[dim];\n-\n     float[] inputUpdates = new float[numInputs * dim];\n     float[] outputUpdates = new float[numOutputs * dim];\n-\n     Int2IntOpenHashMap inputIndex = new Int2IntOpenHashMap();\n     Int2IntOpenHashMap outputIndex = new Int2IntOpenHashMap();\n-\n     Int2IntOpenHashMap inputUpdateCounter = new Int2IntOpenHashMap();\n     Int2IntOpenHashMap outputUpdateCounter = new Int2IntOpenHashMap();\n-\n     Random negativeSeed = new Random(seed);\n     int batchSize = dataBuf.readInt();\n-\n     for (int position = 0; position < batchSize; position++) {\n-      int src = dataBuf.readInt();\n-\n-      float[] inputs = layers[src / numNodeOneRow];\n-      int l1 = (src % numNodeOneRow) * dim * 2;\n-\n-      Arrays.fill(neu1e, 0);\n+        int src = dataBuf.readInt();\n+        float[] inputs = layers[src / numNodeOneRow];\n+        int l1 = (src % numNodeOneRow) * dim * 2;\n+        Arrays.fill(neu1e, 0);\n \n       // Negative sampling\n-      int target;\n-      for (int d = 0; d < negative + 1; d++) {\n-        if (d == 0) target = dataBuf.readInt();\n-        else do {\n-          target = negativeSeed.nextInt(maxIndex);\n-        } while (target == src);\n \n-        float[] outputs = layers[target / numNodeOneRow];\n-        int l2 = (target % numNodeOneRow) * dim * 2 + dim;\n-\n-        float g = dataBuf.readFloat();\n+        int target;\n+        for (int d = 0; d < negative + 1; d++) {\n+                                                                 if (d == 0) target = dataBuf.readInt();\n+                                                                 else\n+                                                                             do {\n+                                                   target = negativeSeed.nextInt(maxIndex);\n+                                                                             } while (target == src);\n+                                                                 float[] outputs = layers[target / numNodeOneRow];\n+                                                                 int l2 = (target % numNodeOneRow) * dim * 2 + dim;\n+                                                                 float g = dataBuf.readFloat();\n \n         // accumulate for the hidden layer\n-        for (int a = 0; a < dim; a++) neu1e[a] += g * outputs[a + l2];\n+                                                                 for (int a = 0; a < dim; a++)\n+                                                   neu1e[a] += g * outputs[a + l2];\n         // update output layer\n-        merge(outputUpdates, outputIndex, target, inputs, g, l1);\n-        outputUpdateCounter.addTo(target, 1);\n-      }\n+                                                                 merge(outputUpdates, outputIndex, target, inputs, g, l1);\n+                                                                 outputUpdateCounter.addTo(target, 1);\n+        }\n \n       // update the hidden layer\n-      merge(inputUpdates, inputIndex, src, neu1e, 1, 0);\n-      inputUpdateCounter.addTo(src, 1);\n+        merge(inputUpdates, inputIndex, src, neu1e, 1, 0);\n+        inputUpdateCounter.addTo(src, 1);\n     }\n \n     // update input\n+\n     ObjectIterator<Int2IntMap.Entry> it = inputIndex.int2IntEntrySet().fastIterator();\n     while (it.hasNext()) {\n-      Int2IntMap.Entry entry = it.next();\n-      int node = entry.getIntKey();\n-      int offset = entry.getIntValue() * dim;\n-      int divider = inputUpdateCounter.get(node);\n-      int col = (node % numNodeOneRow) * dim * 2;\n-      float[] values = layers[node / numNodeOneRow];\n-      for (int a = 0; a < dim; a++) values[a + col] += inputUpdates[offset + a] / divider;\n+        Int2IntMap.Entry entry = it.next();\n+        int node = entry.getIntKey();\n+        int offset = entry.getIntValue() * dim;\n+        int divider = inputUpdateCounter.get(node);\n+        int col = (node % numNodeOneRow) * dim * 2;\n+        float[] values = layers[node / numNodeOneRow];\n+        for (int a = 0; a < dim; a++)\n+                             values[a + col] += inputUpdates[offset + a] / divider;\n     }\n \n     // update output\n     it = outputIndex.int2IntEntrySet().fastIterator();\n+\n     while (it.hasNext()) {\n-      Int2IntMap.Entry entry = it.next();\n-      int node = entry.getIntKey();\n-      int offset = entry.getIntValue() * dim;\n-      int col = (node % numNodeOneRow) * dim * 2 + dim;\n-      float[] values = layers[node / numNodeOneRow];\n-      int divider = outputUpdateCounter.get(node);\n-      for (int a = 0; a < dim; a++) values[a + col] += outputUpdates[offset + a] / divider;\n+        Int2IntMap.Entry entry = it.next();\n+        int node = entry.getIntKey();\n+        int offset = entry.getIntValue() * dim;\n+        int col = (node % numNodeOneRow) * dim * 2 + dim;\n+        float[] values = layers[node / numNodeOneRow];\n+        int divider = outputUpdateCounter.get(node);\n+        for (int a = 0; a < dim; a++)\n+                             values[a + col] += outputUpdates[offset + a] / divider;\n     }\n-  }\n-}\n+    }\n+\n+}\n\\ No newline at end of file\n",
            "diff_size": 91
        },
        {
            "tool": "styler_random",
            "errors": [
                {
                    "line": "15",
                    "column": "42",
                    "severity": "error",
                    "message": "'(' is preceded with whitespace.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.MethodParamPadCheck"
                }
            ],
            "diff": "diff --git a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/53/LINESecondOrderModel.java b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/styler_random/53/LINESecondOrderModel.java\nindex 664a84e90f..2b760da14d 100644\n--- a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/53/LINESecondOrderModel.java\n+++ b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/styler_random/53/LINESecondOrderModel.java\n@@ -11,124 +11,123 @@ import java.util.Random;\n \n public class LINESecondOrderModel extends EmbeddingModel {\n \n-\n-  public LINESecondOrderModel(int dim, int negative, int seed, int maxIndex, int numNodeOneRow, float[][] layers) {\n-    super(dim, negative, seed, maxIndex, numNodeOneRow, layers);\n-  }\n-\n-  @Override\n-  public float[] dot(ByteBuf edges) {\n-\n-    Random negativeSeed = new Random(seed);\n-    IntOpenHashSet numInputs = new IntOpenHashSet();\n-    IntOpenHashSet numOutputs = new IntOpenHashSet();\n-\n-    int batchSize = edges.readInt();\n-    float[] partialDots = new float[batchSize * (1 + negative)];\n-    int dotInc = 0;\n-    for (int position = 0; position < batchSize; position++) {\n-      int src = edges.readInt();\n-      // Skip-Gram model\n-      float[] inputs = layers[src / numNodeOneRow];\n-      int l1 = (src % numNodeOneRow) * dim * 2;\n-      numInputs.add(src);\n-\n-      // Negative sampling\n-      int target;\n-      for (int a = 0; a < negative + 1; a++) {\n-        if (a == 0) target = edges.readInt();\n-        else do {\n-          target = negativeSeed.nextInt(maxIndex);\n-        } while (target == src);\n-\n-        numOutputs.add(target);\n-        float[] outputs = layers[target / numNodeOneRow];\n-        int l2 = (target % numNodeOneRow) * dim * 2 + dim;\n-        float f = 0.0f;\n-        for (int b = 0; b < dim; b++) f += inputs[l1 + b] * outputs[l2 + b];\n-        partialDots[dotInc++] = f;\n-      }\n-    }\n-\n-    this.numInputsToUpdate = numInputs.size();\n-    this.numOutputsToUpdate = numOutputs.size();\n-\n-    return partialDots;\n-  }\n-\n-  @Override\n-  public void adjust(ByteBuf dataBuf, int numInputs, int numOutputs) {\n-\n-    // used to accumulate the updates for input vectors\n-    float[] neu1e = new float[dim];\n-\n-    float[] inputUpdates = new float[numInputs * dim];\n-    float[] outputUpdates = new float[numOutputs * dim];\n-\n-    Int2IntOpenHashMap inputIndex = new Int2IntOpenHashMap();\n-    Int2IntOpenHashMap outputIndex = new Int2IntOpenHashMap();\n-\n-    Int2IntOpenHashMap inputUpdateCounter = new Int2IntOpenHashMap();\n-    Int2IntOpenHashMap outputUpdateCounter = new Int2IntOpenHashMap();\n-\n-    Random negativeSeed = new Random(seed);\n-    int batchSize = dataBuf.readInt();\n-\n-    for (int position = 0; position < batchSize; position++) {\n-      int src = dataBuf.readInt();\n-\n-      float[] inputs = layers[src / numNodeOneRow];\n-      int l1 = (src % numNodeOneRow) * dim * 2;\n-\n-      Arrays.fill(neu1e, 0);\n-\n-      // Negative sampling\n-      int target;\n-      for (int d = 0; d < negative + 1; d++) {\n-        if (d == 0) target = dataBuf.readInt();\n-        else do {\n-          target = negativeSeed.nextInt(maxIndex);\n-        } while (target == src);\n-\n-        float[] outputs = layers[target / numNodeOneRow];\n-        int l2 = (target % numNodeOneRow) * dim * 2 + dim;\n-\n-        float g = dataBuf.readFloat();\n-\n-        // accumulate for the hidden layer\n-        for (int a = 0; a < dim; a++) neu1e[a] += g * outputs[a + l2];\n-        // update output layer\n-        merge(outputUpdates, outputIndex, target, inputs, g, l1);\n-        outputUpdateCounter.addTo(target, 1);\n-      }\n-\n-      // update the hidden layer\n-      merge(inputUpdates, inputIndex, src, neu1e, 1, 0);\n-      inputUpdateCounter.addTo(src, 1);\n-    }\n-\n-    // update input\n-    ObjectIterator<Int2IntMap.Entry> it = inputIndex.int2IntEntrySet().fastIterator();\n-    while (it.hasNext()) {\n-      Int2IntMap.Entry entry = it.next();\n-      int node = entry.getIntKey();\n-      int offset = entry.getIntValue() * dim;\n-      int divider = inputUpdateCounter.get(node);\n-      int col = (node % numNodeOneRow) * dim * 2;\n-      float[] values = layers[node / numNodeOneRow];\n-      for (int a = 0; a < dim; a++) values[a + col] += inputUpdates[offset + a] / divider;\n-    }\n-\n-    // update output\n-    it = outputIndex.int2IntEntrySet().fastIterator();\n-    while (it.hasNext()) {\n-      Int2IntMap.Entry entry = it.next();\n-      int node = entry.getIntKey();\n-      int offset = entry.getIntValue() * dim;\n-      int col = (node % numNodeOneRow) * dim * 2 + dim;\n-      float[] values = layers[node / numNodeOneRow];\n-      int divider = outputUpdateCounter.get(node);\n-      for (int a = 0; a < dim; a++) values[a + col] += outputUpdates[offset + a] / divider;\n-    }\n-  }\n+  public LINESecondOrderModel(int dim, int negative, int seed, int maxIndex, int\n+  numNodeOneRow,float[ ] []layers){super (dim, negative, seed, maxIndex, numNodeOneRow, layers);\n+ }\n+\n+ @Override\n+ public float[] dot(ByteBuf edges) {\n+\n+   Random negativeSeed = new Random(seed);\n+   IntOpenHashSet numInputs = new IntOpenHashSet();\n+   IntOpenHashSet numOutputs = new IntOpenHashSet();\n+\n+   int batchSize = edges.readInt();\n+   float[] partialDots = new float[batchSize * (1 + negative)];\n+   int dotInc = 0;\n+   for (int position = 0; position < batchSize; position++) {\n+     int src = edges.readInt();\n+     // Skip-Gram model\n+     float[] inputs = layers[src / numNodeOneRow];\n+     int l1 = (src % numNodeOneRow) * dim * 2;\n+     numInputs.add(src);\n+\n+     // Negative sampling\n+     int target;\n+     for (int a = 0; a < negative + 1; a++) {\n+       if (a == 0) target = edges.readInt();\n+       else do {\n+         target = negativeSeed.nextInt(maxIndex);\n+       } while (target == src);\n+\n+       numOutputs.add(target);\n+       float[] outputs = layers[target / numNodeOneRow];\n+       int l2 = (target % numNodeOneRow) * dim * 2 + dim;\n+       float f = 0.0f;\n+       for (int b = 0; b < dim; b++) f += inputs[l1 + b] * outputs[l2 + b];\n+       partialDots[dotInc++] = f;\n+     }\n+   }\n+\n+   this.numInputsToUpdate = numInputs.size();\n+   this.numOutputsToUpdate = numOutputs.size();\n+\n+   return partialDots;\n+ }\n+\n+ @Override\n+ public void adjust(ByteBuf dataBuf, int numInputs, int numOutputs) {\n+\n+   // used to accumulate the updates for input vectors\n+   float[] neu1e = new float[dim];\n+\n+   float[] inputUpdates = new float[numInputs * dim];\n+   float[] outputUpdates = new float[numOutputs * dim];\n+\n+   Int2IntOpenHashMap inputIndex = new Int2IntOpenHashMap();\n+   Int2IntOpenHashMap outputIndex = new Int2IntOpenHashMap();\n+\n+   Int2IntOpenHashMap inputUpdateCounter = new Int2IntOpenHashMap();\n+   Int2IntOpenHashMap outputUpdateCounter = new Int2IntOpenHashMap();\n+\n+   Random negativeSeed = new Random(seed);\n+   int batchSize = dataBuf.readInt();\n+\n+   for (int position = 0; position < batchSize; position++) {\n+     int src = dataBuf.readInt();\n+\n+     float[] inputs = layers[src / numNodeOneRow];\n+     int l1 = (src % numNodeOneRow) * dim * 2;\n+\n+     Arrays.fill(neu1e, 0);\n+\n+     // Negative sampling\n+     int target;\n+     for (int d = 0; d < negative + 1; d++) {\n+       if (d == 0) target = dataBuf.readInt();\n+       else do {\n+         target = negativeSeed.nextInt(maxIndex);\n+       } while (target == src);\n+\n+       float[] outputs = layers[target / numNodeOneRow];\n+       int l2 = (target % numNodeOneRow) * dim * 2 + dim;\n+\n+       float g = dataBuf.readFloat();\n+\n+       // accumulate for the hidden layer\n+       for (int a = 0; a < dim; a++) neu1e[a] += g * outputs[a + l2];\n+       // update output layer\n+       merge(outputUpdates, outputIndex, target, inputs, g, l1);\n+       outputUpdateCounter.addTo(target, 1);\n+     }\n+\n+     // update the hidden layer\n+     merge(inputUpdates, inputIndex, src, neu1e, 1, 0);\n+     inputUpdateCounter.addTo(src, 1);\n+   }\n+\n+   // update input\n+   ObjectIterator<Int2IntMap.Entry> it = inputIndex.int2IntEntrySet().fastIterator();\n+   while (it.hasNext()) {\n+     Int2IntMap.Entry entry = it.next();\n+     int node = entry.getIntKey();\n+     int offset = entry.getIntValue() * dim;\n+     int divider = inputUpdateCounter.get(node);\n+     int col = (node % numNodeOneRow) * dim * 2;\n+     float[] values = layers[node / numNodeOneRow];\n+     for (int a = 0; a < dim; a++) values[a + col] += inputUpdates[offset + a] / divider;\n+   }\n+\n+   // update output\n+   it = outputIndex.int2IntEntrySet().fastIterator();\n+   while (it.hasNext()) {\n+     Int2IntMap.Entry entry = it.next();\n+     int node = entry.getIntKey();\n+     int offset = entry.getIntValue() * dim;\n+     int col = (node % numNodeOneRow) * dim * 2 + dim;\n+     float[] values = layers[node / numNodeOneRow];\n+     int divider = outputUpdateCounter.get(node);\n+     for (int a = 0; a < dim; a++) values[a + col] += outputUpdates[offset + a] / divider;\n+   }\n+ }\n }\n",
            "diff_size": 120
        },
        {
            "tool": "styler_three_grams",
            "errors": [
                {
                    "line": "12",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 172).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                }
            ],
            "diff": "diff --git a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/53/LINESecondOrderModel.java b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/styler_three_grams/53/LINESecondOrderModel.java\nindex 664a84e90f..40b7dbfdec 100644\n--- a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/53/LINESecondOrderModel.java\n+++ b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/styler_three_grams/53/LINESecondOrderModel.java\n@@ -9,126 +9,123 @@ import it.unimi.dsi.fastutil.objects.ObjectIterator;\n import java.util.Arrays;\n import java.util.Random;\n \n-public class LINESecondOrderModel extends EmbeddingModel {\n-\n-\n-  public LINESecondOrderModel(int dim, int negative, int seed, int maxIndex, int numNodeOneRow, float[][] layers) {\n-    super(dim, negative, seed, maxIndex, numNodeOneRow, layers);\n-  }\n+public class LINESecondOrderModel extends EmbeddingModel { public LINESecondOrderModel(int dim, int negative, int seed, int maxIndex, int numNodeOneRow, float[][] layers) {\n+  super(dim, negative, seed, maxIndex, numNodeOneRow, layers);\n+}\n \n-  @Override\n-  public float[] dot(ByteBuf edges) {\n-\n-    Random negativeSeed = new Random(seed);\n-    IntOpenHashSet numInputs = new IntOpenHashSet();\n-    IntOpenHashSet numOutputs = new IntOpenHashSet();\n-\n-    int batchSize = edges.readInt();\n-    float[] partialDots = new float[batchSize * (1 + negative)];\n-    int dotInc = 0;\n-    for (int position = 0; position < batchSize; position++) {\n-      int src = edges.readInt();\n-      // Skip-Gram model\n-      float[] inputs = layers[src / numNodeOneRow];\n-      int l1 = (src % numNodeOneRow) * dim * 2;\n-      numInputs.add(src);\n-\n-      // Negative sampling\n-      int target;\n-      for (int a = 0; a < negative + 1; a++) {\n-        if (a == 0) target = edges.readInt();\n-        else do {\n-          target = negativeSeed.nextInt(maxIndex);\n-        } while (target == src);\n-\n-        numOutputs.add(target);\n-        float[] outputs = layers[target / numNodeOneRow];\n-        int l2 = (target % numNodeOneRow) * dim * 2 + dim;\n-        float f = 0.0f;\n-        for (int b = 0; b < dim; b++) f += inputs[l1 + b] * outputs[l2 + b];\n-        partialDots[dotInc++] = f;\n-      }\n+@Override\n+public float[] dot(ByteBuf edges) {\n+\n+  Random negativeSeed = new Random(seed);\n+  IntOpenHashSet numInputs = new IntOpenHashSet();\n+  IntOpenHashSet numOutputs = new IntOpenHashSet();\n+\n+  int batchSize = edges.readInt();\n+  float[] partialDots = new float[batchSize * (1 + negative)];\n+  int dotInc = 0;\n+  for (int position = 0; position < batchSize; position++) {\n+    int src = edges.readInt();\n+    // Skip-Gram model\n+    float[] inputs = layers[src / numNodeOneRow];\n+    int l1 = (src % numNodeOneRow) * dim * 2;\n+    numInputs.add(src);\n+\n+    // Negative sampling\n+    int target;\n+    for (int a = 0; a < negative + 1; a++) {\n+      if (a == 0) target = edges.readInt();\n+      else do {\n+        target = negativeSeed.nextInt(maxIndex);\n+      } while (target == src);\n+\n+      numOutputs.add(target);\n+      float[] outputs = layers[target / numNodeOneRow];\n+      int l2 = (target % numNodeOneRow) * dim * 2 + dim;\n+      float f = 0.0f;\n+      for (int b = 0; b < dim; b++) f += inputs[l1 + b] * outputs[l2 + b];\n+      partialDots[dotInc++] = f;\n     }\n-\n-    this.numInputsToUpdate = numInputs.size();\n-    this.numOutputsToUpdate = numOutputs.size();\n-\n-    return partialDots;\n   }\n \n-  @Override\n-  public void adjust(ByteBuf dataBuf, int numInputs, int numOutputs) {\n+  this.numInputsToUpdate = numInputs.size();\n+  this.numOutputsToUpdate = numOutputs.size();\n+\n+  return partialDots;\n+}\n \n-    // used to accumulate the updates for input vectors\n-    float[] neu1e = new float[dim];\n+@Override\n+public void adjust(ByteBuf dataBuf, int numInputs, int numOutputs) {\n \n-    float[] inputUpdates = new float[numInputs * dim];\n-    float[] outputUpdates = new float[numOutputs * dim];\n+  // used to accumulate the updates for input vectors\n+  float[] neu1e = new float[dim];\n \n-    Int2IntOpenHashMap inputIndex = new Int2IntOpenHashMap();\n-    Int2IntOpenHashMap outputIndex = new Int2IntOpenHashMap();\n+  float[] inputUpdates = new float[numInputs * dim];\n+  float[] outputUpdates = new float[numOutputs * dim];\n \n-    Int2IntOpenHashMap inputUpdateCounter = new Int2IntOpenHashMap();\n-    Int2IntOpenHashMap outputUpdateCounter = new Int2IntOpenHashMap();\n+  Int2IntOpenHashMap inputIndex = new Int2IntOpenHashMap();\n+  Int2IntOpenHashMap outputIndex = new Int2IntOpenHashMap();\n \n-    Random negativeSeed = new Random(seed);\n-    int batchSize = dataBuf.readInt();\n+  Int2IntOpenHashMap inputUpdateCounter = new Int2IntOpenHashMap();\n+  Int2IntOpenHashMap outputUpdateCounter = new Int2IntOpenHashMap();\n \n-    for (int position = 0; position < batchSize; position++) {\n-      int src = dataBuf.readInt();\n+  Random negativeSeed = new Random(seed);\n+  int batchSize = dataBuf.readInt();\n \n-      float[] inputs = layers[src / numNodeOneRow];\n-      int l1 = (src % numNodeOneRow) * dim * 2;\n+  for (int position = 0; position < batchSize; position++) {\n+    int src = dataBuf.readInt();\n \n-      Arrays.fill(neu1e, 0);\n+    float[] inputs = layers[src / numNodeOneRow];\n+    int l1 = (src % numNodeOneRow) * dim * 2;\n \n-      // Negative sampling\n-      int target;\n-      for (int d = 0; d < negative + 1; d++) {\n-        if (d == 0) target = dataBuf.readInt();\n-        else do {\n-          target = negativeSeed.nextInt(maxIndex);\n-        } while (target == src);\n+    Arrays.fill(neu1e, 0);\n \n-        float[] outputs = layers[target / numNodeOneRow];\n-        int l2 = (target % numNodeOneRow) * dim * 2 + dim;\n+    // Negative sampling\n+    int target;\n+    for (int d = 0; d < negative + 1; d++) {\n+      if (d == 0) target = dataBuf.readInt();\n+      else do {\n+        target = negativeSeed.nextInt(maxIndex);\n+      } while (target == src);\n \n-        float g = dataBuf.readFloat();\n+      float[] outputs = layers[target / numNodeOneRow];\n+      int l2 = (target % numNodeOneRow) * dim * 2 + dim;\n \n-        // accumulate for the hidden layer\n-        for (int a = 0; a < dim; a++) neu1e[a] += g * outputs[a + l2];\n-        // update output layer\n-        merge(outputUpdates, outputIndex, target, inputs, g, l1);\n-        outputUpdateCounter.addTo(target, 1);\n-      }\n+      float g = dataBuf.readFloat();\n \n-      // update the hidden layer\n-      merge(inputUpdates, inputIndex, src, neu1e, 1, 0);\n-      inputUpdateCounter.addTo(src, 1);\n+      // accumulate for the hidden layer\n+      for (int a = 0; a < dim; a++) neu1e[a] += g * outputs[a + l2];\n+      // update output layer\n+      merge(outputUpdates, outputIndex, target, inputs, g, l1);\n+      outputUpdateCounter.addTo(target, 1);\n     }\n \n-    // update input\n-    ObjectIterator<Int2IntMap.Entry> it = inputIndex.int2IntEntrySet().fastIterator();\n-    while (it.hasNext()) {\n-      Int2IntMap.Entry entry = it.next();\n-      int node = entry.getIntKey();\n-      int offset = entry.getIntValue() * dim;\n-      int divider = inputUpdateCounter.get(node);\n-      int col = (node % numNodeOneRow) * dim * 2;\n-      float[] values = layers[node / numNodeOneRow];\n-      for (int a = 0; a < dim; a++) values[a + col] += inputUpdates[offset + a] / divider;\n-    }\n+    // update the hidden layer\n+    merge(inputUpdates, inputIndex, src, neu1e, 1, 0);\n+    inputUpdateCounter.addTo(src, 1);\n+  }\n \n-    // update output\n-    it = outputIndex.int2IntEntrySet().fastIterator();\n-    while (it.hasNext()) {\n-      Int2IntMap.Entry entry = it.next();\n-      int node = entry.getIntKey();\n-      int offset = entry.getIntValue() * dim;\n-      int col = (node % numNodeOneRow) * dim * 2 + dim;\n-      float[] values = layers[node / numNodeOneRow];\n-      int divider = outputUpdateCounter.get(node);\n-      for (int a = 0; a < dim; a++) values[a + col] += outputUpdates[offset + a] / divider;\n-    }\n+  // update input\n+  ObjectIterator<Int2IntMap.Entry> it = inputIndex.int2IntEntrySet().fastIterator();\n+  while (it.hasNext()) {\n+    Int2IntMap.Entry entry = it.next();\n+    int node = entry.getIntKey();\n+    int offset = entry.getIntValue() * dim;\n+    int divider = inputUpdateCounter.get(node);\n+    int col = (node % numNodeOneRow) * dim * 2;\n+    float[] values = layers[node / numNodeOneRow];\n+    for (int a = 0; a < dim; a++) values[a + col] += inputUpdates[offset + a] / divider;\n   }\n+\n+  // update output\n+  it = outputIndex.int2IntEntrySet().fastIterator();\n+  while (it.hasNext()) {\n+    Int2IntMap.Entry entry = it.next();\n+    int node = entry.getIntKey();\n+    int offset = entry.getIntValue() * dim;\n+    int col = (node % numNodeOneRow) * dim * 2 + dim;\n+    float[] values = layers[node / numNodeOneRow];\n+    int divider = outputUpdateCounter.get(node);\n+    for (int a = 0; a < dim; a++) values[a + col] += outputUpdates[offset + a] / divider;\n+  }\n+}\n }\n",
            "diff_size": 126
        }
    ]
}