{
    "error_id": "428",
    "information": {
        "errors": [
            {
                "line": "54",
                "severity": "error",
                "message": "No trailing whitespace allowed.",
                "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineCheck"
            }
        ]
    },
    "source_code": "  /**\n   * get gradient over each of predictions, given existing information.\n   * \n   * @param preds: prediction of current round\n   * @param dataMeta information about labels, weights, groups in rank\n   * @param iteration current iteration number. return:_gpair output of get gradient, saves gradient",
    "results": [
        {
            "tool": "styler",
            "errors": [
                {
                    "line": "54",
                    "severity": "error",
                    "message": "No trailing whitespace allowed.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        },
        {
            "tool": "intellij",
            "errors": [
                {
                    "line": "58",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 102).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                }
            ],
            "diff": "diff --git a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/428/SoftmaxMultiClassObj.java b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/intellij/428/SoftmaxMultiClassObj.java\nindex b77a4dd7e5..403d79cbb6 100644\n--- a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/428/SoftmaxMultiClassObj.java\n+++ b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/intellij/428/SoftmaxMultiClassObj.java\n@@ -14,6 +14,7 @@\n  * the License.\n  *\n  */\n+\n package com.tencent.angel.ml.objective;\n \n import com.tencent.angel.ml.RegTree.DataMeta;\n@@ -33,125 +34,125 @@ import java.util.List;\n \n public class SoftmaxMultiClassObj implements ObjFunc {\n \n-  private static final Log LOG = LogFactory.getLog(SoftmaxMultiClassObj.class);\n-\n-  public RegTTrainParam param;\n-  public int numClass;\n-  private boolean outputProb;\n-\n-  public SoftmaxMultiClassObj(RegTTrainParam param, boolean outputProb) {\n-    this.param = param;\n-    this.numClass = param.numClass;\n-    this.outputProb = outputProb;\n-  }\n-\n-  public SoftmaxMultiClassObj(RegTTrainParam param) {\n-    this(param, true);\n-  }\n-\n-  /**\n-   * get gradient over each of predictions, given existing information.\n-   * \n-   * @param preds: prediction of current round\n-   * @param dataMeta information about labels, weights, groups in rank\n-   * @param iteration current iteration number. return:_gpair output of get gradient, saves gradient\n-   *        and second order gradient in\n-   */\n-  @Override\n-  public List<GradPair> getGradient(float[] preds, DataMeta dataMeta, int iteration) {\n-    assert preds.length == this.numClass * dataMeta.labels.length;\n-    List<GradPair> rec = new ArrayList<GradPair>();\n-    int ndata = preds.length / numClass;\n-    int labelError = -1;\n-    float[] tmp = new float[numClass];\n-    for (int insIdx = 0; insIdx < ndata; insIdx++) {\n-      System.arraycopy(preds, insIdx * numClass, tmp, 0, numClass);\n-      MathUtils.softmax(tmp);\n-      int label = (int) dataMeta.labels[insIdx];\n-      if (label < 0 || label >= numClass) {\n-        labelError = label;\n-        label = 0;\n-      }\n-      float wt = dataMeta.getWeight(insIdx);\n-      for (int k = 0; k < numClass; ++k) {\n-        float p = tmp[k];\n-        float h = 2.0f * p * (1.0f - p) * wt;\n-        if (label == k) {\n-          GradPair pair = new GradPair((p - 1.0f) * wt, h);\n-          rec.add(pair);\n-        } else {\n-          GradPair pair = new GradPair(p * wt, h);\n-          rec.add(pair);\n-        }\n-      }\n+    private static final Log LOG = LogFactory.getLog(SoftmaxMultiClassObj.class);\n+\n+    public RegTTrainParam param;\n+    public int numClass;\n+    private boolean outputProb;\n+\n+    public SoftmaxMultiClassObj(RegTTrainParam param, boolean outputProb) {\n+        this.param = param;\n+        this.numClass = param.numClass;\n+        this.outputProb = outputProb;\n     }\n-    if (labelError >= 0 && labelError < numClass) {\n-      LOG.error(String.format(\"SoftmaxMultiClassObj: label must be in [0, num_class), \"\n-          + \"numClass = %d, but found %d in label\", numClass, labelError));\n+\n+    public SoftmaxMultiClassObj(RegTTrainParam param) {\n+        this(param, true);\n     }\n-    return rec;\n-  }\n-\n-  public List<Float> transform(List<Float> preds, boolean prob) {\n-    List<Float> rec = new ArrayList<Float>();\n-    int ndata = preds.size() / numClass;\n-    float[] tmp = new float[numClass];\n-\n-    for (int insIdx = 0; insIdx < ndata; insIdx++) {\n-      for (int k = 0; k < numClass; k++) {\n-        tmp[k] = preds.get(insIdx * numClass + k);\n-      }\n-      if (!prob) {\n-        rec.add((float) MathUtils.findMaxIndex(tmp));\n-      } else {\n-        MathUtils.softmax(tmp);\n-        for (int k = 0; k < numClass; k++) {\n-          preds.set(insIdx * numClass + k, tmp[k]);\n+\n+    /**\n+     * get gradient over each of predictions, given existing information.\n+     *\n+     * @param preds:    prediction of current round\n+     * @param dataMeta  information about labels, weights, groups in rank\n+     * @param iteration current iteration number. return:_gpair output of get gradient, saves gradient\n+     *                  and second order gradient in\n+     */\n+    @Override\n+    public List<GradPair> getGradient(float[] preds, DataMeta dataMeta, int iteration) {\n+        assert preds.length == this.numClass * dataMeta.labels.length;\n+        List<GradPair> rec = new ArrayList<GradPair>();\n+        int ndata = preds.length / numClass;\n+        int labelError = -1;\n+        float[] tmp = new float[numClass];\n+        for (int insIdx = 0; insIdx < ndata; insIdx++) {\n+            System.arraycopy(preds, insIdx * numClass, tmp, 0, numClass);\n+            MathUtils.softmax(tmp);\n+            int label = (int) dataMeta.labels[insIdx];\n+            if (label < 0 || label >= numClass) {\n+                labelError = label;\n+                label = 0;\n+            }\n+            float wt = dataMeta.getWeight(insIdx);\n+            for (int k = 0; k < numClass; ++k) {\n+                float p = tmp[k];\n+                float h = 2.0f * p * (1.0f - p) * wt;\n+                if (label == k) {\n+                    GradPair pair = new GradPair((p - 1.0f) * wt, h);\n+                    rec.add(pair);\n+                } else {\n+                    GradPair pair = new GradPair(p * wt, h);\n+                    rec.add(pair);\n+                }\n+            }\n+        }\n+        if (labelError >= 0 && labelError < numClass) {\n+            LOG.error(String.format(\"SoftmaxMultiClassObj: label must be in [0, num_class), \"\n+                    + \"numClass = %d, but found %d in label\", numClass, labelError));\n         }\n-      }\n+        return rec;\n+    }\n+\n+    public List<Float> transform(List<Float> preds, boolean prob) {\n+        List<Float> rec = new ArrayList<Float>();\n+        int ndata = preds.size() / numClass;\n+        float[] tmp = new float[numClass];\n+\n+        for (int insIdx = 0; insIdx < ndata; insIdx++) {\n+            for (int k = 0; k < numClass; k++) {\n+                tmp[k] = preds.get(insIdx * numClass + k);\n+            }\n+            if (!prob) {\n+                rec.add((float) MathUtils.findMaxIndex(tmp));\n+            } else {\n+                MathUtils.softmax(tmp);\n+                for (int k = 0; k < numClass; k++) {\n+                    preds.set(insIdx * numClass + k, tmp[k]);\n+                }\n+            }\n+        }\n+        // if (!prob) {\n+        // preds.clear();\n+        // preds.addAll(rec);\n+        // }\n+        return rec;\n+    }\n+\n+    @Override\n+    public String defaultEvalMetric() {\n+        return \"merror\";\n+    }\n+\n+    /**\n+     * transform prediction values, this is only called when Prediction is called preds: prediction\n+     * values, saves to this vector as well\n+     *\n+     * @param preds\n+     */\n+    @Override\n+    public void predTransform(List<Float> preds) {\n+        this.transform(preds, this.outputProb);\n+    }\n+\n+    /**\n+     * transform prediction values, this is only called when Eval is called usually it redirect to\n+     * predTransform preds: prediction values, saves to this vector as well\n+     *\n+     * @param preds\n+     */\n+    @Override\n+    public void evalTransform(List<Float> preds) {\n+        this.transform(preds, true);\n+    }\n+\n+    /**\n+     * transform probability value back to margin this is used to transform user-set base_score back\n+     * to margin used by gradient boosting\n+     *\n+     * @param base_score\n+     */\n+    @Override\n+    public float prob2Margin(float base_score) {\n+        return 0;\n     }\n-    // if (!prob) {\n-    // preds.clear();\n-    // preds.addAll(rec);\n-    // }\n-    return rec;\n-  }\n-\n-  @Override\n-  public String defaultEvalMetric() {\n-    return \"merror\";\n-  }\n-\n-  /**\n-   * transform prediction values, this is only called when Prediction is called preds: prediction\n-   * values, saves to this vector as well\n-   *\n-   * @param preds\n-   */\n-  @Override\n-  public void predTransform(List<Float> preds) {\n-    this.transform(preds, this.outputProb);\n-  }\n-\n-  /**\n-   * transform prediction values, this is only called when Eval is called usually it redirect to\n-   * predTransform preds: prediction values, saves to this vector as well\n-   *\n-   * @param preds\n-   */\n-  @Override\n-  public void evalTransform(List<Float> preds) {\n-    this.transform(preds, true);\n-  }\n-\n-  /**\n-   * transform probability value back to margin this is used to transform user-set base_score back\n-   * to margin used by gradient boosting\n-   *\n-   * @param base_score\n-   */\n-  @Override\n-  public float prob2Margin(float base_score) {\n-    return 0;\n-  }\n }\n",
            "diff_size": 203
        },
        {
            "tool": "naturalize",
            "errors": [
                {
                    "line": "54",
                    "severity": "error",
                    "message": "No trailing whitespace allowed.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineCheck"
                }
            ],
            "diff": "diff --git a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/428/SoftmaxMultiClassObj.java b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/naturalize/428/SoftmaxMultiClassObj.java\nindex b77a4dd7e5..15404b2efb 100644\n--- a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/428/SoftmaxMultiClassObj.java\n+++ b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/naturalize/428/SoftmaxMultiClassObj.java\n@@ -88,7 +88,7 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n     if (labelError >= 0 && labelError < numClass) {\n       LOG.error(String.format(\"SoftmaxMultiClassObj: label must be in [0, num_class), \"\n           + \"numClass = %d, but found %d in label\", numClass, labelError));\n-    }\n+  }\n     return rec;\n   }\n \n@@ -154,4 +154,4 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n   public float prob2Margin(float base_score) {\n     return 0;\n   }\n-}\n+}\n\\ No newline at end of file\n",
            "diff_size": 2
        },
        {
            "tool": "codebuff",
            "errors": [
                {
                    "line": "51",
                    "severity": "error",
                    "message": "Block comment has incorrect indentation level 2, expected is 4, indentation should be the same level as line 60.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.CommentsIndentationCheck"
                },
                {
                    "line": "53",
                    "severity": "error",
                    "message": "No trailing whitespace allowed.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineCheck"
                },
                {
                    "line": "91",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 155).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "126",
                    "severity": "error",
                    "message": "Block comment has incorrect indentation level 2, expected is 4, indentation should be the same level as line 133.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.CommentsIndentationCheck"
                },
                {
                    "line": "138",
                    "severity": "error",
                    "message": "Block comment has incorrect indentation level 2, expected is 4, indentation should be the same level as line 145.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.CommentsIndentationCheck"
                },
                {
                    "line": "150",
                    "severity": "error",
                    "message": "Block comment has incorrect indentation level 2, expected is 4, indentation should be the same level as line 157.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.CommentsIndentationCheck"
                }
            ],
            "diff": "diff --git a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/428/SoftmaxMultiClassObj.java b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/codebuff/428/SoftmaxMultiClassObj.java\nindex b77a4dd7e5..ad4b47d60a 100644\n--- a/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/errored/1/428/SoftmaxMultiClassObj.java\n+++ b/home/thomas/mnt/fernanda/styler-test2/python/./experiments/projects/../results/Angel-ML-angel/codebuff/428/SoftmaxMultiClassObj.java\n@@ -22,7 +22,6 @@ import com.tencent.angel.ml.param.RegTTrainParam;\n import com.tencent.angel.ml.utils.MathUtils;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-\n import java.util.ArrayList;\n import java.util.List;\n \n@@ -32,22 +31,22 @@ import java.util.List;\n  */\n \n public class SoftmaxMultiClassObj implements ObjFunc {\n+    private static final Log LOG = LogFactory.getLog(SoftmaxMultiClassObj.class);\n \n-  private static final Log LOG = LogFactory.getLog(SoftmaxMultiClassObj.class);\n+    public RegTTrainParam param;\n \n-  public RegTTrainParam param;\n-  public int numClass;\n-  private boolean outputProb;\n+    public int numClass;\n+    private boolean outputProb;\n \n-  public SoftmaxMultiClassObj(RegTTrainParam param, boolean outputProb) {\n+    public SoftmaxMultiClassObj(RegTTrainParam param, boolean outputProb) {\n     this.param = param;\n     this.numClass = param.numClass;\n     this.outputProb = outputProb;\n-  }\n+    }\n \n-  public SoftmaxMultiClassObj(RegTTrainParam param) {\n+    public SoftmaxMultiClassObj(RegTTrainParam param) {\n     this(param, true);\n-  }\n+    }\n \n   /**\n    * get gradient over each of predictions, given existing information.\n@@ -57,70 +56,72 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n    * @param iteration current iteration number. return:_gpair output of get gradient, saves gradient\n    *        and second order gradient in\n    */\n-  @Override\n-  public List<GradPair> getGradient(float[] preds, DataMeta dataMeta, int iteration) {\n+\n+    @Override\n+    public List<GradPair> getGradient(float[] preds, DataMeta dataMeta, int iteration) {\n     assert preds.length == this.numClass * dataMeta.labels.length;\n     List<GradPair> rec = new ArrayList<GradPair>();\n     int ndata = preds.length / numClass;\n     int labelError = -1;\n     float[] tmp = new float[numClass];\n     for (int insIdx = 0; insIdx < ndata; insIdx++) {\n-      System.arraycopy(preds, insIdx * numClass, tmp, 0, numClass);\n-      MathUtils.softmax(tmp);\n-      int label = (int) dataMeta.labels[insIdx];\n-      if (label < 0 || label >= numClass) {\n+        System.arraycopy(preds, insIdx * numClass, tmp, 0, numClass);\n+        MathUtils.softmax(tmp);\n+\n+        int label = (int) dataMeta.labels[insIdx];\n+        if (label < 0 || label >= numClass) {\n         labelError = label;\n         label = 0;\n-      }\n-      float wt = dataMeta.getWeight(insIdx);\n-      for (int k = 0; k < numClass; ++k) {\n-        float p = tmp[k];\n-        float h = 2.0f * p * (1.0f - p) * wt;\n-        if (label == k) {\n-          GradPair pair = new GradPair((p - 1.0f) * wt, h);\n-          rec.add(pair);\n-        } else {\n-          GradPair pair = new GradPair(p * wt, h);\n-          rec.add(pair);\n         }\n-      }\n+        float wt = dataMeta.getWeight(insIdx);\n+        for (int k = 0; k < numClass; ++k) {\n+                                                       float p = tmp[k];\n+                                                       float h = 2.0f * p * (1.0f - p) * wt;\n+                                                       if (label == k) {\n+            GradPair pair = new GradPair((p - 1.0f) * wt, h);\n+            rec.add(pair);\n+                                                       } else {\n+                                                         GradPair pair = new GradPair(p * wt, h);\n+                                                         rec.add(pair);\n+                                                       }\n+        }\n     }\n+\n     if (labelError >= 0 && labelError < numClass) {\n-      LOG.error(String.format(\"SoftmaxMultiClassObj: label must be in [0, num_class), \"\n-          + \"numClass = %d, but found %d in label\", numClass, labelError));\n+        LOG.error(String.format(\"SoftmaxMultiClassObj: label must be in [0, num_class), \" + \"numClass = %d, but found %d in label\", numClass, labelError));\n     }\n     return rec;\n-  }\n+    }\n \n-  public List<Float> transform(List<Float> preds, boolean prob) {\n+    public List<Float> transform(List<Float> preds, boolean prob) {\n     List<Float> rec = new ArrayList<Float>();\n     int ndata = preds.size() / numClass;\n     float[] tmp = new float[numClass];\n-\n     for (int insIdx = 0; insIdx < ndata; insIdx++) {\n-      for (int k = 0; k < numClass; k++) {\n-        tmp[k] = preds.get(insIdx * numClass + k);\n-      }\n-      if (!prob) {\n-        rec.add((float) MathUtils.findMaxIndex(tmp));\n-      } else {\n-        MathUtils.softmax(tmp);\n         for (int k = 0; k < numClass; k++) {\n-          preds.set(insIdx * numClass + k, tmp[k]);\n+                                                       tmp[k] = preds.get(insIdx * numClass + k);\n+        }\n+\n+        if (!prob) {\n+        rec.add((float) MathUtils.findMaxIndex(tmp));\n+        } else {\n+          MathUtils.softmax(tmp);\n+          for (int k = 0; k < numClass; k++) {\n+                   preds.set(insIdx * numClass + k, tmp[k]);\n+          }\n         }\n-      }\n     }\n     // if (!prob) {\n     // preds.clear();\n     // preds.addAll(rec);\n     // }\n     return rec;\n-  }\n+    }\n \n-  @Override\n-  public String defaultEvalMetric() {\n+    @Override\n+    public String defaultEvalMetric() {\n     return \"merror\";\n-  }\n+    }\n \n   /**\n    * transform prediction values, this is only called when Prediction is called preds: prediction\n@@ -128,10 +129,11 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n    *\n    * @param preds\n    */\n-  @Override\n-  public void predTransform(List<Float> preds) {\n+\n+    @Override\n+    public void predTransform(List<Float> preds) {\n     this.transform(preds, this.outputProb);\n-  }\n+    }\n \n   /**\n    * transform prediction values, this is only called when Eval is called usually it redirect to\n@@ -139,10 +141,11 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n    *\n    * @param preds\n    */\n-  @Override\n-  public void evalTransform(List<Float> preds) {\n+\n+    @Override\n+    public void evalTransform(List<Float> preds) {\n     this.transform(preds, true);\n-  }\n+    }\n \n   /**\n    * transform probability value back to margin this is used to transform user-set base_score back\n@@ -150,8 +153,9 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n    *\n    * @param base_score\n    */\n-  @Override\n-  public float prob2Margin(float base_score) {\n+\n+    @Override\n+    public float prob2Margin(float base_score) {\n     return 0;\n-  }\n-}\n+    }\n+}\n\\ No newline at end of file\n",
            "diff_size": 82
        },
        {
            "tool": "styler_random",
            "errors": [
                {
                    "line": "54",
                    "severity": "error",
                    "message": "No trailing whitespace allowed.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        },
        {
            "tool": "styler_three_grams",
            "errors": [
                {
                    "line": "54",
                    "severity": "error",
                    "message": "No trailing whitespace allowed.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        }
    ]
}